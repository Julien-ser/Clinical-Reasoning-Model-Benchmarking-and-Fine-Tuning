{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Reasoning Model Benchmarking and Fine-Tuning (MedCalc-Bench)\n",
    "\n",
    "This notebook benchmarks and fine-tunes Qwen3 models on MedCalc-Bench clinical calculation tasks. It covers: prompt engineering (zero-shot, few-shot, CoT), optional advanced methods, parameter-efficient fine-tuning (LoRA/QLoRA), and category-wise evaluation.\n",
    "\n",
    "Use this on Google Colab Free Tier or locally. All steps are reproducible and parameter choices are explained.\n",
    "\n",
    "## Contents\n",
    "- Setup and data loading\n",
    "- Prompt engineering: Zero-shot, Few-shot, CoT (+ optional advanced)\n",
    "- Fine-tuning with LoRA/QLoRA (PEFT)\n",
    "- Inference and evaluation\n",
    "- Results visualization and interpretation\n",
    "\n",
    "Tip: If you are in Colab, switch to T4 GPU in Runtime > Change runtime type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility and Setup\n",
    "\n",
    "- Run the setup cell below on Colab Free Tier (T4) or locally.\n",
    "- This notebook assumes result CSVs are in the repository root and LoRA adapters under `qwen_lora_*/`.\n",
    "- For full fine-tuning, ensure GPU runtime is enabled.\n",
    "\n",
    "Parameters and decisions are documented inline before each major block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-interactive install for Colab/local\n",
    "# - -q keeps output terse in notebooks\n",
    "# - Pin transformers >=4.43.0 for Qwen3 compatibility and latest generate features\n",
    "# - bitsandbytes enables 8-bit/4-bit loading for memory savings\n",
    "# - accelerate/peft support device placement and parameter-efficient fine-tuning\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q \"transformers>=4.43.0\" accelerate peft bitsandbytes datasets evaluate scikit-learn seaborn matplotlib pandas numpy\n",
    "!pip install -q einops xformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Structure\n",
    "\n",
    "We use MedCalc-Bench with fields: `Patient Note`, `Question`, `Category`, `Relevant Entities` (JSON), `Ground Truth Answer`, `Ground Truth Explanation`.\n",
    "\n",
    "- Train file: `dataset/train_data.csv`\n",
    "- Test file: `dataset/test_data.csv`\n",
    "\n",
    "If you are only reproducing results, skip to the Results section; otherwise load the dataset for fine-tuning/inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "id": "rRmDapWqLQ3G",
    "outputId": "8bfd35f2-114f-44ac-a255-6448c9b8e703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (10053, 14)\n",
      "Test data shape: (1047, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row Number</th>\n",
       "      <th>Calculator ID</th>\n",
       "      <th>Calculator Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Output Type</th>\n",
       "      <th>Note ID</th>\n",
       "      <th>Note Type</th>\n",
       "      <th>Patient Note</th>\n",
       "      <th>Question</th>\n",
       "      <th>Relevant Entities</th>\n",
       "      <th>Ground Truth Answer</th>\n",
       "      <th>Lower Limit</th>\n",
       "      <th>Upper Limit</th>\n",
       "      <th>Ground Truth Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Creatinine Clearance (Cockcroft-Gault Equation)</td>\n",
       "      <td>lab</td>\n",
       "      <td>decimal</td>\n",
       "      <td>pmc-7671985-1</td>\n",
       "      <td>Extracted</td>\n",
       "      <td>An 87-year-old man was admitted to our hospita...</td>\n",
       "      <td>What is the patient's Creatinine Clearance usi...</td>\n",
       "      <td>{'sex': 'Male', 'age': [87, 'years'], 'weight'...</td>\n",
       "      <td>25.238</td>\n",
       "      <td>23.9761</td>\n",
       "      <td>26.4999</td>\n",
       "      <td>The formula for computing Cockcroft-Gault is g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Creatinine Clearance (Cockcroft-Gault Equation)</td>\n",
       "      <td>lab</td>\n",
       "      <td>decimal</td>\n",
       "      <td>pmc-8605939-1</td>\n",
       "      <td>Extracted</td>\n",
       "      <td>An 83-year-old man with a past medical history...</td>\n",
       "      <td>What is the patient's Creatinine Clearance usi...</td>\n",
       "      <td>{'sex': 'Male', 'age': [83, 'years'], 'weight'...</td>\n",
       "      <td>38</td>\n",
       "      <td>36.1</td>\n",
       "      <td>39.9</td>\n",
       "      <td>The formula for computing Cockcroft-Gault is g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Creatinine Clearance (Cockcroft-Gault Equation)</td>\n",
       "      <td>lab</td>\n",
       "      <td>decimal</td>\n",
       "      <td>pmc-6482549-1</td>\n",
       "      <td>Extracted</td>\n",
       "      <td>A 51-year-old woman who presented with diarrho...</td>\n",
       "      <td>What is the patient's Creatinine Clearance usi...</td>\n",
       "      <td>{'sex': 'Female', 'age': [51, 'years'], 'weigh...</td>\n",
       "      <td>25.017</td>\n",
       "      <td>23.76615</td>\n",
       "      <td>26.26785</td>\n",
       "      <td>The formula for computing Cockcroft-Gault is g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Creatinine Clearance (Cockcroft-Gault Equation)</td>\n",
       "      <td>lab</td>\n",
       "      <td>decimal</td>\n",
       "      <td>usmle-1002</td>\n",
       "      <td>Extracted</td>\n",
       "      <td>A 42-year-old woman comes to the physician for...</td>\n",
       "      <td>What is the patient's Creatinine Clearance usi...</td>\n",
       "      <td>{'sex': 'Female', 'age': [42, 'years'], 'weigh...</td>\n",
       "      <td>106.192</td>\n",
       "      <td>100.8824</td>\n",
       "      <td>111.5016</td>\n",
       "      <td>The formula for computing Cockcroft-Gault is g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Creatinine Clearance (Cockcroft-Gault Equation)</td>\n",
       "      <td>lab</td>\n",
       "      <td>decimal</td>\n",
       "      <td>pmc-4459668-1</td>\n",
       "      <td>Extracted</td>\n",
       "      <td>A 45-year-old, 58 kg, 156 cm woman presented w...</td>\n",
       "      <td>What is the patient's Creatinine Clearance usi...</td>\n",
       "      <td>{'sex': 'Female', 'age': [45, 'years'], 'weigh...</td>\n",
       "      <td>78.121</td>\n",
       "      <td>74.21495</td>\n",
       "      <td>82.02705</td>\n",
       "      <td>The formula for computing Cockcroft-Gault is g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row Number  Calculator ID                                  Calculator Name  \\\n",
       "0           1              2  Creatinine Clearance (Cockcroft-Gault Equation)   \n",
       "1           2              2  Creatinine Clearance (Cockcroft-Gault Equation)   \n",
       "2           3              2  Creatinine Clearance (Cockcroft-Gault Equation)   \n",
       "3           4              2  Creatinine Clearance (Cockcroft-Gault Equation)   \n",
       "4           5              2  Creatinine Clearance (Cockcroft-Gault Equation)   \n",
       "\n",
       "  Category Output Type        Note ID  Note Type  \\\n",
       "0      lab     decimal  pmc-7671985-1  Extracted   \n",
       "1      lab     decimal  pmc-8605939-1  Extracted   \n",
       "2      lab     decimal  pmc-6482549-1  Extracted   \n",
       "3      lab     decimal     usmle-1002  Extracted   \n",
       "4      lab     decimal  pmc-4459668-1  Extracted   \n",
       "\n",
       "                                        Patient Note  \\\n",
       "0  An 87-year-old man was admitted to our hospita...   \n",
       "1  An 83-year-old man with a past medical history...   \n",
       "2  A 51-year-old woman who presented with diarrho...   \n",
       "3  A 42-year-old woman comes to the physician for...   \n",
       "4  A 45-year-old, 58 kg, 156 cm woman presented w...   \n",
       "\n",
       "                                            Question  \\\n",
       "0  What is the patient's Creatinine Clearance usi...   \n",
       "1  What is the patient's Creatinine Clearance usi...   \n",
       "2  What is the patient's Creatinine Clearance usi...   \n",
       "3  What is the patient's Creatinine Clearance usi...   \n",
       "4  What is the patient's Creatinine Clearance usi...   \n",
       "\n",
       "                                   Relevant Entities Ground Truth Answer  \\\n",
       "0  {'sex': 'Male', 'age': [87, 'years'], 'weight'...              25.238   \n",
       "1  {'sex': 'Male', 'age': [83, 'years'], 'weight'...                  38   \n",
       "2  {'sex': 'Female', 'age': [51, 'years'], 'weigh...              25.017   \n",
       "3  {'sex': 'Female', 'age': [42, 'years'], 'weigh...             106.192   \n",
       "4  {'sex': 'Female', 'age': [45, 'years'], 'weigh...              78.121   \n",
       "\n",
       "  Lower Limit Upper Limit                           Ground Truth Explanation  \n",
       "0     23.9761     26.4999  The formula for computing Cockcroft-Gault is g...  \n",
       "1        36.1        39.9  The formula for computing Cockcroft-Gault is g...  \n",
       "2    23.76615    26.26785  The formula for computing Cockcroft-Gault is g...  \n",
       "3    100.8824    111.5016  The formula for computing Cockcroft-Gault is g...  \n",
       "4    74.21495    82.02705  The formula for computing Cockcroft-Gault is g...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load MedCalc-Bench directly from Hugging Face\n",
    "dataset = load_dataset(\"ncbi/MedCalc-Bench-v1.0\")\n",
    "\n",
    "# Split into train/test\n",
    "train_df = dataset[\"train\"].to_pandas()\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "# Preview\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "#train_df.head()\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Output Types: ['decimal' 'integer' 'date']\n",
      "\n",
      "Sample for Output Type: decimal\n",
      "Row Number                                                                  1\n",
      "Calculator ID                                                               2\n",
      "Calculator Name               Creatinine Clearance (Cockcroft-Gault Equation)\n",
      "Category                                                             lab test\n",
      "Output Type                                                           decimal\n",
      "Note ID                                                         pmc-6477550-1\n",
      "Note Type                                                           Extracted\n",
      "Patient Note                A 16-year-old female adolescent was referred t...\n",
      "Question                    What is the patient's Creatinine Clearance usi...\n",
      "Relevant Entities           {'sex': 'Female', 'weight': [55.0, 'kg'], 'hei...\n",
      "Ground Truth Answer                                                   141.042\n",
      "Lower Limit                                                            133.99\n",
      "Upper Limit                                                           148.094\n",
      "Ground Truth Explanation    The formula for computing Cockcroft-Gault is g...\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Sample for Output Type: integer\n",
      "Row Number                                                                637\n",
      "Calculator ID                                                               4\n",
      "Calculator Name             CHA2DS2-VASc Score for Atrial Fibrillation Str...\n",
      "Category                                                                 risk\n",
      "Output Type                                                           integer\n",
      "Note ID                                                         pmc-6132167-1\n",
      "Note Type                                                           Extracted\n",
      "Patient Note                A 78-year-old man visited our emergency room w...\n",
      "Question                            What is the patient's CHA2DS2-VASc Score?\n",
      "Relevant Entities           {'sex': 'Male', 'Thromboembolism history': Tru...\n",
      "Ground Truth Answer                                                         5\n",
      "Lower Limit                                                                 5\n",
      "Upper Limit                                                                 5\n",
      "Ground Truth Explanation    The current CHA2DS2-VASc score is 0.\\nThe pati...\n",
      "Name: 636, dtype: object\n",
      "\n",
      "Sample for Output Type: date\n",
      "Row Number                                                               9654\n",
      "Calculator ID                                                              13\n",
      "Calculator Name                                            Estimated Due Date\n",
      "Category                                                                 date\n",
      "Output Type                                                              date\n",
      "Note ID                                                                    22\n",
      "Note Type                                                            Template\n",
      "Patient Note                The patient's last menstrual period was on 09/...\n",
      "Question                    Using Naegele's Rule for estimated due date ba...\n",
      "Relevant Entities           {'cycle length': 21, 'Last menstrual date': '0...\n",
      "Ground Truth Answer                                                06/30/2012\n",
      "Lower Limit                                                        06/30/2012\n",
      "Upper Limit                                                        06/30/2012\n",
      "Ground Truth Explanation    The patient's estimated due date based on thei...\n",
      "Name: 9653, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Explore distribution of output types to guide prompt formatting and parsing logic\n",
    "# - 'decimal' vs 'integer' steer numeric casting\n",
    "# - 'date' activates MM/DD/YYYY and gestational age tuple handling\n",
    "output_types = train_df[\"Output Type\"].unique()\n",
    "print(\"Unique Output Types:\", output_types)\n",
    "\n",
    "# Show a representative example per type to sanity-check fields and ranges\n",
    "for ot in output_types:\n",
    "    print(f\"\\nSample for Output Type: {ot}\")\n",
    "    print(train_df[train_df[\"Output Type\"] == ot].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Models\n",
    "\n",
    "We evaluate two Qwen3 sizes for compute/accuracy tradeoffs:\n",
    "- **Qwen3-0.6B**: fast, low VRAM, strong baseline with prompt engineering\n",
    "- **Qwen3-1.7B**: higher capacity, better reasoning, still Colab-compatible with optimizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tT0tsDpkiqkq"
   },
   "outputs": [],
   "source": [
    "# Model names mapped to Hugging Face hub identifiers\n",
    "# Rationale:\n",
    "# - Two sizes cover speed/accuracy trade-offs and fit on common GPUs (T4/A10)\n",
    "# - Keeping a dict enables uniform loops for loading/evaluation\n",
    "model_names = {\n",
    "    \"0.6B\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"1.7B\": \"Qwen/Qwen3-1.7B\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Tokenizer Loading (Qwen3-0.6B, Qwen3-1.7B)\n",
    "\n",
    "- Left-padding/truncation for decoder-only models to align attention masks.\n",
    "- bfloat16 weights if available; `device_map='auto'` to distribute on GPU.\n",
    "- Loads both models to compare prompt methods fairly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVfGw7ebLQ3U",
    "outputId": "68d6cc6d-f2f8-43ab-b657-fc171a1c20f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen3-0.6B...\n",
      "Loading Qwen/Qwen3-1.7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0e7cb6bf464237a0b9da0ca609c725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded:\n",
      "0.6B Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ")\n",
      "1.7B Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load models and tokenizers with settings optimized for decoder-only Qwen3\n",
    "# Parameter choices and reasoning:\n",
    "# - padding_side = \"left\": Enables efficient batching for decoder-only LMs where attention\n",
    "#   depends on position from the right; avoids shifting attention masks across batch items.\n",
    "# - truncation_side = \"left\": Keeps the most recent and typically most relevant context\n",
    "#   when sequences exceed max length (useful for long patient notes).\n",
    "# - torch_dtype = torch.bfloat16: Cuts memory roughly in half compared to fp32 while\n",
    "#   maintaining numerical stability on modern GPUs (A100, T4 w/ emulation). Falls back\n",
    "#   to fp16 automatically at runtime if bfloat16 is unsupported.\n",
    "# - device_map = \"auto\": Automatically places weights on available GPU(s) to prevent OOM\n",
    "#   and leverages VRAM without manual layer placement.\n",
    "# - trust_remote_code = True: Required for some community models (like Qwen) that ship\n",
    "#   custom modeling code; vetted upstream but should be used only with trusted repos.\n",
    "models, tokenizers = {}, {}\n",
    "for key, name in model_names.items():\n",
    "    print(f\"Loading {name}...\")\n",
    "    tokenizers[key] = AutoTokenizer.from_pretrained(name)\n",
    "    tokenizers[key].padding_side = \"left\"   # left-padding for batched decoder-only generation\n",
    "    tokenizers[key].truncation_side = \"left\"  # preserve most recent context under truncation\n",
    "    models[key] = AutoModelForCausalLM.from_pretrained(\n",
    "        name,\n",
    "        torch_dtype=torch.bfloat16,  # memory-efficient weights with stable numerics\n",
    "        device_map=\"auto\",          # auto-shard across GPU(s) if present\n",
    "        trust_remote_code=True       # allow model-specific implementations\n",
    "    )\n",
    "\n",
    "# Quick check of architectures for sanity (helps ensure correct variant is loaded)\n",
    "print(\"Models loaded:\")\n",
    "for key in models:\n",
    "    print(key, models[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight Generator Wrapper\n",
    "\n",
    "- Encodes prompt and generates a short completion.\n",
    "- Conservative decoding defaults keep outputs concise and reproducible across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWuWp5xKLQ3W"
   },
   "outputs": [],
   "source": [
    "def generate_answer(prompt, model, tokenizer, max_length=32):\n",
    "    \"\"\"Lightweight wrapper around generate for quick single-prompt checks.\n",
    "    - Keeps temperature low to reduce variance while allowing minor exploration.\n",
    "    - Uses left-padding defaults already set on tokenizer.\n",
    "    - max_length defaults to 32 to encourage concise answers (e.g., numbers/dates).\n",
    "    \"\"\"\n",
    "    # Tokenize to tensors and place on GPU for faster inference\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Sampling choices:\n",
    "    # - do_sample=True with temperature=0.3: allows slight exploration while keeping outputs stable\n",
    "    # - top_p=0.9: nucleus sampling trims tail probabilities to avoid degenerate loops\n",
    "    # - eos_token_id/pad_token_id: ensure proper early stopping and consistent padding behavior\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_length,   # cap new tokens to keep outputs short and on-task\n",
    "        do_sample=True,\n",
    "        temperature=0.3,             # low randomness for reproducible numeric outputs\n",
    "        top_p=0.9,                   # constrain to high-probability tokens\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Convert token IDs back to string, skipping any special tokens that may appear\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Extraction, Parsing, and Validation\n",
    "\n",
    "- Robust `Answer:` extraction from free-form generations.\n",
    "- Category-aware parsing (numeric vs. date vs. gestational age tuples).\n",
    "- Range validation against provided lower/upper limits; this drives accuracy metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MNwlI-MLQ3Z"
   },
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Answer Extraction and Validation Functions\n",
    "# -------------------\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def extract_answer_from_output(output_text):\n",
    "    \"\"\"Extract the last occurrence of 'Answer:' from model output.\n",
    "    Rationale: models may emit multiple 'Answer:' lines (e.g., retries or CoT).\n",
    "    Choosing the last occurrence biases toward the final corrected answer.\n",
    "    \"\"\"\n",
    "    # Find all occurrences of \"Answer:\"\n",
    "    answer_matches = re.findall(r'Answer:\\s*(.+?)(?:\\n|$)', output_text, re.IGNORECASE)\n",
    "    if answer_matches:\n",
    "        last_answer = answer_matches[-1].strip()\n",
    "        # Remove any additional text after a newline just in case\n",
    "        last_answer = last_answer.split('\\n')[0].strip()\n",
    "        return last_answer\n",
    "    \n",
    "    return \"N/A\"\n",
    "\n",
    "def parse_answer(answer_str, category):\n",
    "    \"\"\"Parse answer string based on category.\n",
    "    Decisions:\n",
    "    - Treat 'unknown' and NaN as missing to avoid false positives.\n",
    "    - For numeric categories, prefer float when decimals present; otherwise cast to int\n",
    "      to align with evaluation expectations for discrete scores.\n",
    "    - For dates, standardize to MM/DD/YYYY where possible and support (weeks, days)\n",
    "      tuples for gestational age style answers.\n",
    "    \"\"\"\n",
    "    if pd.isna(answer_str) or answer_str == \"unknown\":\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "            # These categories typically expect numeric values\n",
    "            # Check if it's an integer or decimal\n",
    "            if '.' in str(answer_str):\n",
    "                return float(answer_str)\n",
    "            else:\n",
    "                return int(float(answer_str))\n",
    "        elif category == \"date\":\n",
    "            # First check for (weeks, days) format\n",
    "            weeks_days_match = re.search(r'\\((\\d+)\\s*weeks?[,\\s]*[\\'\"]?\\s*(\\d+)\\s*days?[\\'\"]?\\)', str(answer_str), re.IGNORECASE)\n",
    "            if weeks_days_match:\n",
    "                weeks = int(weeks_days_match.group(1))\n",
    "                days = int(weeks_days_match.group(2))\n",
    "                # Return as tuple for exact matching in validation\n",
    "                return (weeks, days)\n",
    "            \n",
    "            # Try to parse various date formats, prioritizing MM/DD/YYYY\n",
    "            date_formats = [\n",
    "                \"%m/%d/%Y\", \"%m/%d/%y\",  # MM/DD/YYYY, MM/DD/YY\n",
    "                \"%Y-%m-%d\", \"%Y/%m/%d\",  # YYYY-MM-DD, YYYY/MM/DD\n",
    "                \"%d/%m/%Y\", \"%d/%m/%y\",  # DD/MM/YYYY, DD/MM/YY\n",
    "                \"%Y-%m\", \"%Y\", \"%m/%Y\", \"%Y/%m\"  # Partial dates\n",
    "            ]\n",
    "            for fmt in date_formats:\n",
    "                try:\n",
    "                    parsed_date = datetime.strptime(str(answer_str), fmt)\n",
    "                    return parsed_date.strftime(\"%m/%d/%Y\")  # Standardize to MM/DD/YYYY\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            # If no format matches, try to extract year\n",
    "            year_match = re.search(r'(\\d{4})', str(answer_str))\n",
    "            if year_match:\n",
    "                return f\"01/01/{year_match.group(1)}\"\n",
    "            return None\n",
    "        else:\n",
    "            return answer_str\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def validate_answer_range(answer, lower_limit, upper_limit, category):\n",
    "    \"\"\"Check if answer falls within the expected range.\n",
    "    Rationale:\n",
    "    - Returns False on missing inputs to keep the metric conservative.\n",
    "    - Numeric categories: inclusive range check after float casting to tolerate\n",
    "      integer/float formatting differences.\n",
    "    - Date category: exact tuple match for (weeks, days); otherwise inclusive\n",
    "      range check on standardized MM/DD/YYYY strings.\n",
    "    \"\"\"\n",
    "    if answer is None or pd.isna(lower_limit) or pd.isna(upper_limit):\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        if category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "            answer_val = float(answer)\n",
    "            lower_val = float(lower_limit)\n",
    "            upper_val = float(upper_limit)\n",
    "            return lower_val <= answer_val <= upper_val\n",
    "        elif category == \"date\":\n",
    "            # Handle (weeks, days) format - exact match required\n",
    "            if isinstance(answer, tuple) and len(answer) == 2:\n",
    "                # This is a (weeks, days) format - check for exact match\n",
    "                try:\n",
    "                    # Parse the ground truth to see if it's also in (weeks, days) format\n",
    "                    gt_weeks_days_match = re.search(r'\\((\\d+)\\s*weeks?[,\\s]*[\\'\"]?\\s*(\\d+)\\s*days?[\\'\"]?\\)', str(lower_limit), re.IGNORECASE)\n",
    "                    if gt_weeks_days_match:\n",
    "                        gt_weeks = int(gt_weeks_days_match.group(1))\n",
    "                        gt_days = int(gt_weeks_days_match.group(2))\n",
    "                        return answer == (gt_weeks, gt_days)\n",
    "                    else:\n",
    "                        # If ground truth is not in (weeks, days) format, no match\n",
    "                        return False\n",
    "                except (ValueError, TypeError):\n",
    "                    return False\n",
    "            \n",
    "            # For regular dates, we need to parse them\n",
    "            try:\n",
    "                answer_date = datetime.strptime(str(answer), \"%m/%d/%Y\")\n",
    "                lower_date = datetime.strptime(str(lower_limit), \"%m/%d/%Y\")\n",
    "                upper_date = datetime.strptime(str(upper_limit), \"%m/%d/%Y\")\n",
    "                return lower_date <= answer_date <= upper_date\n",
    "            except ValueError:\n",
    "                return False\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "# -------------------\n",
    "# Enhanced Zero-Shot Prompt with Better Date Handling\n",
    "# -------------------\n",
    "def zero_shot_prompt(patient_note, question, entities_json=None, category=None):\n",
    "    entities_section = f\"Relevant Entities: {entities_json}\\n\\n\" if entities_json else \"\"\n",
    "    \n",
    "    # Determine format instructions based on category\n",
    "    if category == \"date\":\n",
    "        format_instructions = \"\"\"- For dates, use MM/DD/YYYY format (e.g., 08/31/2023)\n",
    "- If only year is available, use 01/01/YYYY\n",
    "- If year and month are available, use MM/01/YYYY\n",
    "- For gestational age or time periods, use (X weeks, Y days) format (e.g., (0 weeks, 6 days))\n",
    "- Extract the most specific date information available\"\"\"\n",
    "    elif category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "        format_instructions = \"\"\"- For numeric values, provide precise values with appropriate decimal places\n",
    "- Round to 2-3 decimal places unless more precision is needed\n",
    "- Use standard decimal notation (e.g., 123.45)\n",
    "- For whole numbers, provide exact integer values\"\"\"\n",
    "    else:\n",
    "        format_instructions = \"\"\"- For numbers, use appropriate precision (integers for whole numbers, decimals for precise values)\n",
    "- For dates, use MM/DD/YYYY format\"\"\"\n",
    "    \n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a clinical calculation assistant. Extract only the numerical answer or date from the patient case.\n",
    "- Output format: Answer: <number or date only>\n",
    "- Do not explain, justify, or add any text\n",
    "- Do not repeat the question\n",
    "- {format_instructions}\n",
    "- If unsure, provide your best numerical estimate\n",
    "- Ensure your answer falls within reasonable clinical ranges<|im_end|>\n",
    "<|im_start|>user\n",
    "{entities_section}Patient case:\n",
    "{patient_note}\n",
    "\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer:\"\"\"\n",
    "\n",
    "# -------------------\n",
    "# Enhanced Few-Shot Prompt with Better Examples\n",
    "# -------------------\n",
    "def few_shot_prompt(patient_note, question, examples, n, entities_json=None, category=None):\n",
    "    # Determine format instructions based on category\n",
    "    if category == \"date\":\n",
    "        format_instructions = \"\"\"- For dates, use MM/DD/YYYY format (e.g., 08/31/2023)\n",
    "- If only year is available, use 01/01/YYYY\n",
    "- If year and month are available, use MM/01/YYYY\n",
    "- For gestational age or time periods, use (X weeks, Y days) format (e.g., (0 weeks, 6 days))\n",
    "- Extract the most specific date information available\"\"\"\n",
    "    elif category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "        format_instructions = \"\"\"- For numeric values, provide precise values with appropriate decimal places\n",
    "- Round to 2-3 decimal places unless more precision is needed\n",
    "- Use standard decimal notation (e.g., 123.45)\n",
    "- For whole numbers, provide exact integer values\"\"\"\n",
    "    else:\n",
    "        format_instructions = \"\"\"- For numbers, use appropriate precision (integers for whole numbers, decimals for precise values)\n",
    "- For dates, use MM/DD/YYYY format\"\"\"\n",
    "    \n",
    "    prompt_text = f\"\"\"<|im_start|>system\n",
    "You are a clinical calculation assistant. Extract only the numerical answer or date from the patient case.\n",
    "- Output format: Answer: <number or date only>\n",
    "- Do not explain, justify, or add any text\n",
    "- Do not repeat the question\n",
    "- {format_instructions}\n",
    "- Follow the exact format of the examples below\n",
    "- Ensure your answer falls within reasonable clinical ranges<|im_end|>\n",
    "\"\"\"\n",
    "    \n",
    "    for i in range(min(n, len(examples))):\n",
    "        ex = examples.iloc[i]\n",
    "        # Format the ground truth answer properly\n",
    "        gt_answer = ex['Ground Truth Answer']\n",
    "        if pd.isna(gt_answer) or str(gt_answer).strip() == \"\":\n",
    "            gt_answer_str = \"unknown\"\n",
    "        else:\n",
    "            gt_answer_str = str(gt_answer).strip()\n",
    "        \n",
    "        prompt_text += f\"\"\"<|im_start|>user\n",
    "Patient case:\n",
    "{ex['Patient Note']}\n",
    "\n",
    "Question: {ex['Question']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer: {gt_answer_str}<|im_end|>\n",
    "\"\"\"\n",
    "    \n",
    "    entities_section = f\"Relevant Entities: {entities_json}\\n\\n\" if entities_json else \"\"\n",
    "    prompt_text += f\"\"\"<|im_start|>user\n",
    "{entities_section}Patient case:\n",
    "{patient_note}\n",
    "\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer:\"\"\"\n",
    "    return prompt_text\n",
    "\n",
    "# -------------------\n",
    "# Enhanced Chain-of-Thought (CoT) Prompt with Better Reasoning\n",
    "# -------------------\n",
    "def CoT(patient_note, question, entities_json=None, category=None):\n",
    "    entities_section = f\"Relevant Entities: {entities_json}\\n\\n\" if entities_json else \"\"\n",
    "    \n",
    "    # Determine format instructions based on category\n",
    "    if category == \"date\":\n",
    "        format_instructions = \"\"\"- For dates, use MM/DD/YYYY format (e.g., 08/31/2023)\n",
    "- If only year is available, use 01/01/YYYY\n",
    "- If year and month are available, use MM/01/YYYY\n",
    "- For gestational age or time periods, use (X weeks, Y days) format (e.g., (0 weeks, 6 days))\n",
    "- Extract the most specific date information available\"\"\"\n",
    "    elif category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "        format_instructions = \"\"\"- For numeric values, provide precise values with appropriate decimal places\n",
    "- Round to 2-3 decimal places unless more precision is needed\n",
    "- Use standard decimal notation (e.g., 123.45)\n",
    "- For whole numbers, provide exact integer values\"\"\"\n",
    "    else:\n",
    "        format_instructions = \"\"\"- For numbers, use appropriate precision (integers for whole numbers, decimals for precise values)\n",
    "- For dates, use MM/DD/YYYY format\"\"\"\n",
    "    \n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a clinical calculation assistant. First provide the final answer, then your reasoning inside <thinking> tags.\n",
    "- Output format: \n",
    "Answer: <number or date only>\n",
    "<thinking>step-by-step reasoning here</thinking>\n",
    "- Do not include any text outside this format\n",
    "- Keep reasoning concise and focused on calculation steps\n",
    "- {format_instructions}\n",
    "- Ensure your answer falls within reasonable clinical ranges<|im_end|>\n",
    "<|im_start|>user\n",
    "{entities_section}Patient case:\n",
    "{patient_note}\n",
    "\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Sanity Outputs by Category\n",
    "\n",
    "- For a single example per category, prints raw generations for each model and method.\n",
    "- Useful to spot prompt formatting issues early before full-batch runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "_dLQeAwOLQ3c",
    "outputId": "648f054e-8979-4639-a8e8-b3799c063b62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc17641f79ed4367a2e41eeafd9eb3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Categories:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9339c5e8cb34132a281b0bf18d040b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models (lab test):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c74a5dd39c9421f930a65f7637b866f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models (risk):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578b02988a72485b881deffa6139f831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models (physical):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b0592c21684984b094dee57aa9b574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models (severity):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26c302a2dce44f7bd8bf9d6fd92b9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models (diagnosis):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f5a276efee4be1ac8373be20617e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models (date):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4009bf4da97f42dd89900dd0e7221ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models (dosage conversion):   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "categories = train_df['Category'].unique()\n",
    "\n",
    "with open(\"model_outputs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for category in tqdm(categories, desc=\"Categories\"):\n",
    "        # Select a representative example for this category\n",
    "        example = train_df[train_df['Category'] == category].iloc[0]\n",
    "        \n",
    "        f.write(f\"\\n##### CATEGORY: {category} #####\\n\\n\")\n",
    "        for model_key in tqdm([\"1.7B\", \"0.6B\"], desc=f\"Models ({category})\", leave=False):\n",
    "            f.write(f\"===== MODEL {model_key} =====\\n\\n\")\n",
    "\n",
    "            # Zero-shot\n",
    "            zs_prompt = zero_shot_prompt(\n",
    "                example['Patient Note'], \n",
    "                example['Question'], \n",
    "                example[\"Relevant Entities\"]\n",
    "            )\n",
    "            zs_output = generate_answer(zs_prompt, models[model_key], tokenizers[model_key])\n",
    "            f.write(\"Zero-shot output:\\n\")\n",
    "            f.write(zs_output + \"\\n\")\n",
    "            f.write(\"------------------------------\\n\\n\")\n",
    "\n",
    "            # Few-shot\n",
    "            fs_prompt = few_shot_prompt(\n",
    "                example['Patient Note'], \n",
    "                example['Question'], \n",
    "                train_df, 3,\n",
    "                example[\"Relevant Entities\"]\n",
    "            )\n",
    "            fs_output = generate_answer(fs_prompt, models[model_key], tokenizers[model_key])\n",
    "            f.write(\"Few-shot output:\\n\")\n",
    "            f.write(fs_output + \"\\n\")\n",
    "            f.write(\"------------------------------\\n\\n\")\n",
    "\n",
    "            # CoT\n",
    "            cot_prompt = CoT(\n",
    "                example['Patient Note'], \n",
    "                example['Question'], \n",
    "                example[\"Relevant Entities\"]\n",
    "            )\n",
    "            cot_output = generate_answer(cot_prompt, models[model_key], tokenizers[model_key], max_length=256)\n",
    "            f.write(\"CoT output:\\n\")\n",
    "            f.write(cot_output + \"\\n\")\n",
    "            f.write(\"==============================\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched Inference over Test Set\n",
    "\n",
    "- Runs all methods (zero-shot, few-shot, CoT) for both models.\n",
    "- Batching with left-padding to support decoder-only models.\n",
    "- Sampling: `temperature=0.3`, `top_p=0.9` to allow slight diversity but keep outputs stable.\n",
    "- Extracts `Answer:` from generations, parses by category, and validates within ground-truth range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492,
     "referenced_widgets": [
      "e6e7ca43d838432bb6d10b13d5a947fe",
      "dd08890a2bcc4bea9e1d8f295ad48c9d",
      "5a186ceda6324233860986498222e2ae",
      "acae790e32c74a82bd4c8b05f463e1f9",
      "2bbc6e995c1f4f63be50f5e9dad6e376",
      "2193aa8aa26f4f6098638667ff4c23e2",
      "fce5a57cb99049fa9fe98c4c9cf94c15",
      "28b118d7850d41d7a1ad8d473adbb3be",
      "b0f347bff23b40ae900077846b5441e7",
      "bebd4dc4632d48b784a1f2910a533114",
      "9186461e305f4e80ba48cbd1897d7688"
     ]
    },
    "id": "9mGMpAwxLQ3d",
    "outputId": "bba6f6af-4923-4df7-eb20-226ab1dcab17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving → /outputs/0.6B_zero_shot_improved.csv\n",
      "Loading model Qwen/Qwen3-0.6B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52ef258bd00408c86147c0ea90b8774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.6B-zero_shot:   0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving → /outputs/1.7B_zero_shot_improved.csv\n",
      "Loading model Qwen/Qwen3-1.7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e631b8a4b7774890a5038096487057c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7dbeeb0116421eaa4cfdd15d347aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1.7B-zero_shot:   0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving → /outputs/0.6B_cot_improved.csv\n",
      "Loading model Qwen/Qwen3-0.6B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e5aebf13344756823bc513641c432d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.6B-cot:   0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving → /outputs/1.7B_cot_improved.csv\n",
      "Loading model Qwen/Qwen3-1.7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3a7c7fd1cf41d384a99025acff5450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca71a309e3d485ba987fb9ff4742929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1.7B-cot:   0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving → /outputs/0.6B_few_shot_improved.csv\n",
      "Loading model Qwen/Qwen3-0.6B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633820086001453dafb4440c351a9833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.6B-few_shot:   0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving → /outputs/1.7B_few_shot_improved.csv\n",
      "Loading model Qwen/Qwen3-1.7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f493f7beb95543e4b5abd3d3d063881c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad5e642311246a68cb6ddde09f5e073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1.7B-few_shot:   0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "OUTPUT_DIR = \"/outputs\"  # absolute-like path; works in Colab/workspace root mounted with write perms\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Few-shot K balances context budget vs information signal; increase if VRAM allows\n",
    "N_FEW_SHOT = 3  # chosen to fit alongside long notes without truncation on T4\n",
    "# Batch size is VRAM-bound; 4 is typically safe on T4 for modest context windows\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def generate_batch(prompts, model, tokenizer, max_length=512):\n",
    "    \"\"\"Generate answers for a batch of prompts with controlled sampling.\n",
    "    - Left-padded inputs allow efficient batching.\n",
    "    - Low temperature keeps outputs stable; top_p avoids degenerate loops.\n",
    "    - max_length=512 accommodates CoT where needed while keeping GPU time bounded.\n",
    "    \"\"\"\n",
    "    # Tokenize a list of prompts with padding/truncation for batched generation\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    # Decoding parameters mirror single-prompt wrapper to maintain consistency\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_length,   # cap new tokens to control latency/VRAM\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "def run_inference(df, model_key, prompt_type, out_file, use_sampling=True, temperature=0.7):\n",
    "    \"\"\"Run dataset-wide inference for a given model and prompting method.\n",
    "    - Builds prompts per row using category-aware templates.\n",
    "    - Extracts `Answer:` then parses/validates into accuracy via `in_range`.\n",
    "    Decisions:\n",
    "    - Reload model per run to avoid cross-prompt-type interference and free VRAM in between.\n",
    "    - Left-padding and left-truncation align with earlier tokenizer setup for batching.\n",
    "    - `temperature` argument kept for future tuning hooks; internal default remains 0.3.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model {model_names[model_key]}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_names[model_key])\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_names[model_key]).to(\"cuda\").eval()\n",
    "\n",
    "    rows = []\n",
    "    batch_prompts = []\n",
    "    batch_indices = []\n",
    "\n",
    "    # Precompute all prompts first (reduces CPU/GPU idle time)\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{model_key}-{prompt_type}\"):\n",
    "        patient_note = row[\"Patient Note\"]\n",
    "        question = row[\"Question\"]\n",
    "        entities = row[\"Relevant Entities\"]\n",
    "        category = row.get(\"Category\", None)\n",
    "\n",
    "        if prompt_type == \"zero_shot\":\n",
    "            prompt = zero_shot_prompt(patient_note, question, entities, category)\n",
    "        elif prompt_type == \"few_shot\":\n",
    "            prompt = few_shot_prompt(patient_note, question, df, N_FEW_SHOT, entities, category)\n",
    "        elif prompt_type == \"cot\":\n",
    "            prompt = CoT(patient_note, question, entities, category)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prompt_type: {prompt_type}\")\n",
    "\n",
    "        batch_prompts.append(prompt)\n",
    "        batch_indices.append(idx)\n",
    "\n",
    "        # Generate in batches at fixed size to control VRAM\n",
    "        if len(batch_prompts) == BATCH_SIZE:\n",
    "            if prompt_type == \"cot\":\n",
    "                outputs = generate_batch(batch_prompts, model, tokenizer, max_length=256)\n",
    "            else:\n",
    "                outputs = generate_batch(batch_prompts, model, tokenizer)\n",
    "\n",
    "            for i, output in zip(batch_indices, outputs):\n",
    "                # Extract numeric/date answer and compute correctness\n",
    "                raw_answer = extract_answer_from_output(output)\n",
    "                parsed_answer = parse_answer(raw_answer, category)\n",
    "                in_range = validate_answer_range(\n",
    "                    parsed_answer,\n",
    "                    df.loc[i, \"Lower Limit\"],\n",
    "                    df.loc[i, \"Upper Limit\"],\n",
    "                    category,\n",
    "                )\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"index\": i,\n",
    "                        \"id\": df.loc[i, \"Note ID\"],\n",
    "                        \"model\": model_key,\n",
    "                        \"prompt_type\": prompt_type,\n",
    "                        \"category\": df.loc[i, \"Category\"],\n",
    "                        \"patient_note\": df.loc[i, \"Patient Note\"],\n",
    "                        \"question\": df.loc[i, \"Question\"],\n",
    "                        \"entities\": df.loc[i, \"Relevant Entities\"],\n",
    "                        \"ground_truth\": df.loc[i, \"Ground Truth Answer\"],\n",
    "                        \"lower_limit\": df.loc[i, \"Lower Limit\"],\n",
    "                        \"upper_limit\": df.loc[i, \"Upper Limit\"],\n",
    "                        \"raw_output\": output,\n",
    "                        \"extracted_answer\": raw_answer,\n",
    "                        \"parsed_answer\": parsed_answer,\n",
    "                        \"in_range\": in_range,\n",
    "                    }\n",
    "                )\n",
    "            batch_prompts, batch_indices = [], []\n",
    "\n",
    "    # Process remaining prompts (tail batch)\n",
    "    if batch_prompts:\n",
    "        if prompt_type == \"cot\":\n",
    "            outputs = generate_batch(batch_prompts, model, tokenizer, max_length=256)\n",
    "        else:\n",
    "            outputs = generate_batch(batch_prompts, model, tokenizer)\n",
    "\n",
    "        for i, output in zip(batch_indices, outputs):\n",
    "            raw_answer = extract_answer_from_output(output)\n",
    "            parsed_answer = parse_answer(raw_answer, category)\n",
    "            in_range = validate_answer_range(\n",
    "                parsed_answer,\n",
    "                df.loc[i, \"Lower Limit\"],\n",
    "                df.loc[i, \"Upper Limit\"],\n",
    "                category,\n",
    "            )\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"index\": i,\n",
    "                    \"id\": df.loc[i, \"Note ID\"],\n",
    "                    \"model\": model_key,\n",
    "                    \"prompt_type\": prompt_type,\n",
    "                    \"category\": df.loc[i, \"Category\"],\n",
    "                    \"patient_note\": df.loc[i, \"Patient Note\"],\n",
    "                    \"question\": df.loc[i, \"Question\"],\n",
    "                    \"entities\": df.loc[i, \"Relevant Entities\"],\n",
    "                    \"ground_truth\": df.loc[i, \"Ground Truth Answer\"],\n",
    "                    \"lower_limit\": df.loc[i, \"Lower Limit\"],\n",
    "                    \"upper_limit\": df.loc[i, \"Upper Limit\"],\n",
    "                    \"raw_output\": output,\n",
    "                    \"extracted_answer\": raw_answer,\n",
    "                    \"parsed_answer\": parsed_answer,\n",
    "                    \"in_range\": in_range,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Save results to CSV for downstream aggregation\n",
    "    pd.DataFrame(rows).to_csv(out_file, index=False)\n",
    "\n",
    "    # Unload model and clear cache to avoid OOM in subsequent runs\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Run all combinations with improved parameters for both models\n",
    "for prompt_type in [\"zero_shot\", \"cot\", \"few_shot\"]:\n",
    "    for model_key in [\"0.6B\", \"1.7B\"]:\n",
    "        out_file = f\"{OUTPUT_DIR}/{model_key}_{prompt_type}_improved.csv\"\n",
    "        if os.path.exists(out_file):\n",
    "            print(f\"Skipping → {out_file} (already exists)\")\n",
    "            continue\n",
    "        print(f\"Saving → {out_file}\")\n",
    "        run_inference(\n",
    "            test_df,\n",
    "            model_key,\n",
    "            prompt_type,\n",
    "            out_file,\n",
    "            use_sampling=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Builders: Zero-shot, Few-shot, CoT\n",
    "\n",
    "- Zero-shot: strict answer formatting, category-specific output instructions.\n",
    "- Few-shot: embeds K labeled exemplars; K tuned for context/VRAM.\n",
    "- CoT: emits `Answer:` then concise steps in `<thinking>` tags to capture reasoning, while evaluation uses only the extracted answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGIOTAixRK8l",
    "outputId": "861b561d-2eaf-4627-f049-133d7c74c67b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.3.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch<3,>=2.2->bitsandbytes) (68.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT and Quantization Config\n",
    "\n",
    "- BitsAndBytes 4-bit (NF4 + double quant) to minimize VRAM usage.\n",
    "- This enables QLoRA: train adapters while keeping base weights 4-bit quantized.\n",
    "- Compute dtype float16 for speed/memory balance on T4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jJG3372zLQ3g"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Define your 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Formatting and Tokenization\n",
    "\n",
    "- Builds zero-shot training pairs with strict output format `Answer: <value>`.\n",
    "- For dates: normalizes to MM/DD/YYYY or (weeks, days) when present.\n",
    "- Masks prompt tokens in `labels` as -100 so loss focuses on target answer only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JAbNd0UMLQ3h"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "def get_peft_config(model_name):\n",
    "    return LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "def build_zero_shot(example, tokenizer=None):\n",
    "    patient_note = example[\"Patient Note\"]\n",
    "    question = example[\"Question\"]\n",
    "    answer = example[\"Ground Truth Answer\"]\n",
    "    entities = example.get(\"Relevant Entities\", \"\")\n",
    "    category = example.get(\"Category\", None)\n",
    "\n",
    "    entities_section = f\"Relevant Entities: {entities}\\n\\n\" if entities else \"\"\n",
    "\n",
    "    if category == \"date\":\n",
    "        format_instructions = \"- For dates, use MM/DD/YYYY format\\n- If only year is available, use 01/01/YYYY\\n- For gestational age, use (X weeks, Y days)\"\n",
    "    elif category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "        format_instructions = \"- Provide precise numeric values\\n- Round to 2-3 decimal places unless more precision is needed\\n- Use standard decimal notation\"\n",
    "    else:\n",
    "        format_instructions = \"- For numbers, use appropriate precision\\n- For dates, use MM/DD/YYYY\\n- For gestational age, use (X weeks, Y days)\"\n",
    "\n",
    "    input_text = f\"\"\"<|im_start|>system\n",
    "You are a clinical calculation assistant. Extract only the numerical answer or date.\n",
    "- Output format: Answer: <number or date only>\n",
    "- Do not explain or add any text\n",
    "- {format_instructions}\n",
    "- Ensure your answer is clinically reasonable<|im_end|>\n",
    "<|im_start|>user\n",
    "{entities_section}Patient case:\n",
    "{patient_note}\n",
    "\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "    if pd.isna(answer) or str(answer).strip() == \"\":\n",
    "        formatted_answer = \"unknown\"\n",
    "    else:\n",
    "        if category == \"date\":\n",
    "            try:\n",
    "                match = re.search(r'\\((\\d+)\\s*weeks?[, ]*(\\d+)\\s*days?\\)', str(answer), re.IGNORECASE)\n",
    "                if match:\n",
    "                    weeks, days = map(int, match.groups())\n",
    "                    formatted_answer = f\"({weeks} weeks, {days} days)\"\n",
    "                else:\n",
    "                    date_formats = [\"%m/%d/%Y\", \"%m/%d/%y\", \"%Y-%m-%d\", \"%d/%m/%Y\", \"%Y\"]\n",
    "                    parsed_date = None\n",
    "                    for fmt in date_formats:\n",
    "                        try:\n",
    "                            parsed_date = datetime.strptime(str(answer), fmt)\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "                    if parsed_date:\n",
    "                        formatted_answer = parsed_date.strftime(\"%m/%d/%Y\")\n",
    "                    else:\n",
    "                        year_match = re.search(r'(\\d{4})', str(answer))\n",
    "                        formatted_answer = f\"01/01/{year_match.group(1)}\" if year_match else str(answer).strip()\n",
    "            except:\n",
    "                formatted_answer = str(answer).strip()\n",
    "        else:\n",
    "            formatted_answer = str(answer).strip()\n",
    "\n",
    "    label_answer = f\"Answer: {formatted_answer}\"\n",
    "    return {\"input_text\": input_text.strip(), \"labels\": label_answer.strip()}\n",
    "\n",
    "def tokenize_function(example, tokenizer):\n",
    "    zero_shot = build_zero_shot(example, tokenizer)\n",
    "    input_ids = tokenizer(zero_shot[\"input_text\"], add_special_tokens=True)[\"input_ids\"]\n",
    "    labels = tokenizer(zero_shot[\"labels\"], add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # Mask the prompt tokens with -100\n",
    "    prompt_len = len(input_ids)\n",
    "    input_ids += labels\n",
    "    labels = [-100] * prompt_len + labels\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Loop (PEFT + QLoRA)\n",
    "\n",
    "- Applies LoRA on attention/MLP projections with r=16, alpha=16, dropout=0.05.\n",
    "- Uses 4-bit NF4 quantization for memory efficiency (QLoRA).\n",
    "- Tokenization masks the prompt portion (labels=-100) to train only on the target string.\n",
    "- TrainingArgs tuned for stability under limited VRAM (batch 4, grad_accum 8, 3 epochs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531,
     "referenced_widgets": [
      "e4862d4872a1492088c2a3a0d943ae74",
      "9d20d912668245d483fb35e33a2d9555",
      "6ef7346f78a444a282edf65a4c2479e6",
      "4daff3b9a2af4cdcb26300d230ecfc94",
      "801b36af9e7045e596f8b719cef3e3f0",
      "ccac27e3c2bf45e7af1e81fe577e32ed",
      "7217115a6f304797919d81a23968b486",
      "ae1bc63b4e8c42b896f6c9999bcde18b",
      "2def2041cb114c1db533ead31a234748",
      "cd985c75884a4ba5992726c376a0f0b8",
      "23524eeb480b48dca2d7be051d60ef5a"
     ]
    },
    "id": "lgFYtEpfLQ3i",
    "outputId": "abeb8349-f748-4b98-eb60-644dcac9bb90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Qwen/Qwen3-0.6B with LoRA/QLoRA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='945' max='945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [945/945 1:24:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.690700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.919500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.915100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.905400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.900900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished training Qwen/Qwen3-0.6B\n",
      "LoRA adapter saved to qwen_lora_0.6B/adapter\n",
      "Tokenizer saved to ./qwen_lora_0.6B/tokenizer\n",
      "\n",
      "=== Training Qwen/Qwen3-1.7B with LoRA/QLoRA ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a359341abbb54352b89a28a20df1f7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d54ba36cb8448f798c2325a840c712a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='945' max='945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [945/945 2:21:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.873300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.857500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.817400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.850700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished training Qwen/Qwen3-1.7B\n",
      "LoRA adapter saved to qwen_lora_1.7B/adapter\n",
      "Tokenizer saved to ./qwen_lora_1.7B/tokenizer\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "train_hf = dataset[\"train\"]\n",
    "\n",
    "# Safe collator that handles missing attention_mask or labels\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch]\n",
    "    # Create attention_mask as 1 where input_ids != pad_token\n",
    "    attention_mask = [torch.ones_like(ids, dtype=torch.long) for ids in input_ids]\n",
    "\n",
    "    # If labels exist, use them; otherwise mask everything\n",
    "    labels = []\n",
    "    for x in batch:\n",
    "        if \"labels\" in x:\n",
    "            labels.append(torch.tensor(x[\"labels\"], dtype=torch.long))\n",
    "        else:\n",
    "            labels.append(torch.full_like(torch.tensor(x[\"input_ids\"], dtype=torch.long), -100))\n",
    "\n",
    "    # Pad sequences\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "for key, name in model_names.items():\n",
    "    print(f\"\\n=== Training {name} with LoRA/QLoRA ===\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    # Apply LoRA\n",
    "    peft_config = get_peft_config(name)\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_dataset = train_hf.map(\n",
    "        lambda x: tokenize_function(x, tokenizer),\n",
    "        batched=False\n",
    "    )\n",
    "\n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./qwen_lora_{key}\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        save_steps=100,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=collate_fn\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    print(f\"✅ Finished training {name}\")\n",
    "\n",
    "    # Save LoRA adapter\n",
    "    lora_save_path = Path(f\"./qwen_lora_{key}/adapter\")\n",
    "    lora_save_path.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(lora_save_path)\n",
    "    print(f\"LoRA adapter saved to {lora_save_path}\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(f\"./qwen_lora_{key}/tokenizer\")\n",
    "    print(f\"Tokenizer saved to ./qwen_lora_{key}/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized Inference with LoRA Adapters (QLoRA)\n",
    "\n",
    "- Loads 4-bit quantized base model and merges trained LoRA adapters for inference.\n",
    "- Rationale: fit within Colab T4 memory while retaining fine-tuned behavior.\n",
    "- Method: Zero-shot prompting on merged model; outputs saved per model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "e585f8a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving → /outputs/0.6B_zero_shot_quantized_lora.csv\n",
      "Loading base quantized model Qwen/Qwen3-0.6B...\n",
      "Loading tokenizer from ./qwen_lora_0.6B/tokenizer...\n",
      "Loading LoRA adapter from ./qwen_lora_0.6B/adapter...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbacc4fa5cc4ad68ec83619986b8b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.6B-quantized:   0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving → /outputs/1.7B_zero_shot_quantized_lora.csv\n",
      "Loading base quantized model Qwen/Qwen3-1.7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d7933064c04981882aa021d1a86a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from ./qwen_lora_1.7B/tokenizer...\n",
      "Loading LoRA adapter from ./qwen_lora_1.7B/adapter...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1631ecbababf4ece8eba71049942821e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1.7B-quantized:   0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run inference on quantized models with saved LoRA adapters\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Define your 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = \"/outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "N_FEW_SHOT = 5\n",
    "BATCH_SIZE = 4  # Adjust depending on your GPU memory\n",
    "\n",
    "def generate_batch(prompts, model, tokenizer, max_new_tokens=512):\n",
    "    \"\"\"Generate answers for a batch of prompts without cutting off.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=False\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=None,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "def run_quantized_inference_from_lora(df, model_key, out_file):\n",
    "    # Load base model with quantization\n",
    "    print(f\"Loading base quantized model {model_names[model_key]}...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_names[model_key],\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    # Load tokenizer from saved LoRA path\n",
    "    tokenizer_path = f\"./qwen_lora_{model_key}/tokenizer\"\n",
    "    print(f\"Loading tokenizer from {tokenizer_path}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "\n",
    "    # Load LoRA adapter\n",
    "    lora_path = f\"./qwen_lora_{model_key}/adapter\"\n",
    "    print(f\"Loading LoRA adapter from {lora_path}...\")\n",
    "    model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    model = model.merge_and_unload()  # Merge LoRA layers for inference\n",
    "    model.eval()\n",
    "\n",
    "    rows = []\n",
    "    batch_prompts = []\n",
    "    batch_indices = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{model_key}-quantized\"):\n",
    "        zero_shot_example = build_zero_shot(row)\n",
    "        prompt = zero_shot_example[\"input_text\"]\n",
    "\n",
    "        batch_prompts.append(prompt)\n",
    "        batch_indices.append(idx)\n",
    "\n",
    "        if len(batch_prompts) == BATCH_SIZE:\n",
    "            outputs = generate_batch(batch_prompts, model, tokenizer)\n",
    "            for i, output in zip(batch_indices, outputs):\n",
    "                raw_answer = extract_answer_from_output(output)\n",
    "                parsed_answer = parse_answer(raw_answer, df.loc[i, \"Category\"])\n",
    "                in_range = validate_answer_range(parsed_answer, \n",
    "                                                 df.loc[i, \"Lower Limit\"], \n",
    "                                                 df.loc[i, \"Upper Limit\"], \n",
    "                                                 df.loc[i, \"Category\"])\n",
    "                rows.append({\n",
    "                    \"index\": i,\n",
    "                    \"id\": df.loc[i, \"Note ID\"],\n",
    "                    \"model\": model_key,\n",
    "                    \"prompt_type\": \"zero_shot_quantized_lora\",\n",
    "                    \"category\": df.loc[i, \"Category\"],\n",
    "                    \"patient_note\": df.loc[i, \"Patient Note\"],\n",
    "                    \"question\": df.loc[i, \"Question\"],\n",
    "                    \"entities\": df.loc[i, \"Relevant Entities\"],\n",
    "                    \"ground_truth\": df.loc[i, \"Ground Truth Answer\"],\n",
    "                    \"lower_limit\": df.loc[i, \"Lower Limit\"],\n",
    "                    \"upper_limit\": df.loc[i, \"Upper Limit\"],\n",
    "                    \"raw_output\": output,\n",
    "                    \"extracted_answer\": raw_answer,\n",
    "                    \"parsed_answer\": parsed_answer,\n",
    "                    \"in_range\": in_range\n",
    "                })\n",
    "            batch_prompts, batch_indices = [], []\n",
    "\n",
    "    # Process remaining prompts\n",
    "    if batch_prompts:\n",
    "        outputs = generate_batch(batch_prompts, model, tokenizer)\n",
    "        for i, output in zip(batch_indices, outputs):\n",
    "            raw_answer = extract_answer_from_output(output)\n",
    "            parsed_answer = parse_answer(raw_answer, df.loc[i, \"Category\"])\n",
    "            in_range = validate_answer_range(parsed_answer, \n",
    "                                             df.loc[i, \"Lower Limit\"], \n",
    "                                             df.loc[i, \"Upper Limit\"], \n",
    "                                             df.loc[i, \"Category\"])\n",
    "            rows.append({\n",
    "                \"index\": i,\n",
    "                \"id\": df.loc[i, \"Note ID\"],\n",
    "                \"model\": model_key,\n",
    "                \"prompt_type\": \"zero_shot_quantized_lora\",\n",
    "                \"category\": df.loc[i, \"Category\"],\n",
    "                \"patient_note\": df.loc[i, \"Patient Note\"],\n",
    "                \"question\": df.loc[i, \"Question\"],\n",
    "                \"entities\": df.loc[i, \"Relevant Entities\"],\n",
    "                \"ground_truth\": df.loc[i, \"Ground Truth Answer\"],\n",
    "                \"lower_limit\": df.loc[i, \"Lower Limit\"],\n",
    "                \"upper_limit\": df.loc[i, \"Upper Limit\"],\n",
    "                \"raw_output\": output,\n",
    "                \"extracted_answer\": raw_answer,\n",
    "                \"parsed_answer\": parsed_answer,\n",
    "                \"in_range\": in_range\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_file, index=False)\n",
    "\n",
    "    # Unload model and clear cache\n",
    "    del model\n",
    "    del base_model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Run inference\n",
    "if 'test_df' in locals():\n",
    "    for model_key in [\"0.6B\", \"1.7B\"]:\n",
    "        out_file = f\"{OUTPUT_DIR}/{model_key}_zero_shot_quantized_lora.csv\"\n",
    "        if os.path.exists(out_file):\n",
    "            print(f\"Skipping → {out_file} (already exists)\")\n",
    "            continue\n",
    "        print(f\"Saving → {out_file}\")\n",
    "        run_quantized_inference_from_lora(test_df, model_key, out_file)\n",
    "else:\n",
    "    print(\"Error: test_df not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "- Embeds train set with `all-MiniLM-L6-v2` and retrieves top-K similar cases.\n",
    "- Builds prompts that include retrieved (note, question, answer) pairs as guidance.\n",
    "- Evaluates both selected Qwen3 models under the RAG prompting scheme.\n",
    "- Parameter choices: `N_RETRIEVAL=5`, moderate `temperature=0.7` with `top_p=0.9` for diversity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving → /outputs/0.6B_rag_improved.csv\n",
      "Loading model Qwen/Qwen3-0.6B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4911af5c7694824bdb97693acd2f8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0.6B-RAG:   0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving → /outputs/1.7B_rag_improved.csv\n",
      "Loading model Qwen/Qwen3-1.7B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500d733c366b4d04a108f45fc811d320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc102279c8f41fd87a45f27fbf86da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1.7B-RAG:   0%|          | 0/1047 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "OUTPUT_DIR = \"/outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "N_RETRIEVAL = 5   # number of neighbors retrieved\n",
    "\n",
    "# --- Load Embedding Model ---\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Load Data ---\n",
    "dataset = load_dataset(\"ncbi/MedCalc-Bench-v1.0\")\n",
    "\n",
    "# Split into train/test\n",
    "train_df = dataset[\"train\"].to_pandas()\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "# --- Precompute Train Embeddings (if not already saved) ---\n",
    "if not os.path.exists(\"rag_train_embeddings.npy\"):\n",
    "    train_texts = (\n",
    "        train_df[\"Patient Note\"] + \" \" +\n",
    "        train_df[\"Question\"] + \" \" +\n",
    "        (train_df[\"Relevant Entities\"].fillna(\"\") if \"Relevant Entities\" in train_df else \"\") + \" \" +\n",
    "        (train_df[\"Ground Truth Explanation\"].fillna(\"\") if \"Ground Truth Explanation\" in train_df else \"\")\n",
    "    )\n",
    "    train_embeddings = embedding_model.encode(train_texts.tolist(), convert_to_numpy=True)\n",
    "    np.save(\"rag_train_embeddings.npy\", train_embeddings)\n",
    "\n",
    "def load_rag_embeddings(embed_file=\"rag_train_embeddings.npy\"):\n",
    "    return np.load(embed_file)\n",
    "\n",
    "# --- Retrieval ---\n",
    "def retrieve_examples(note, question, k=N_RETRIEVAL):\n",
    "    rag_embeds = load_rag_embeddings()\n",
    "    text = note + \" \" + question\n",
    "    test_embed = embedding_model.encode([text], convert_to_numpy=True)\n",
    "    sims = np.dot(rag_embeds, test_embed[0])\n",
    "    idxs = sims.argsort()[-k:][::-1]\n",
    "    return train_df.iloc[idxs]\n",
    "\n",
    "# --- Prompt Builder ---\n",
    "def rag_prompt(patient_note, question, examples, n=None, entities_json=None, category=None, include_explanations=False):\n",
    "    \"\"\"\n",
    "    Build a RAG-style prompt with improved formatting.\n",
    "    - examples: a DataFrame (retrieved examples); will include up to n examples (default: all)\n",
    "    - entities_json: optional string for Relevant Entities\n",
    "    - category: category of the question (lab, risk, physical, severity, diagnosis, date, dosage)\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(examples)\n",
    "\n",
    "    entities_section = f\"Relevant Entities: {entities_json}\\n\\n\" if entities_json else \"\"\n",
    "\n",
    "    # Determine format instructions based on category\n",
    "    if category == \"date\":\n",
    "        format_instructions = \"\"\"- For dates, use MM/DD/YYYY format (e.g., 08/31/2023)\n",
    "- If only year is available, use 01/01/YYYY\n",
    "- If year and month are available, use MM/01/YYYY\n",
    "- For gestational age or time periods, use (X weeks, Y days) format (e.g., (0 weeks, 6 days))\n",
    "- Extract the most specific date information available\"\"\"\n",
    "    elif category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "        format_instructions = \"\"\"- For numeric values, provide precise values with appropriate decimal places\n",
    "- Round to 2-3 decimal places unless more precision is needed\n",
    "- Use standard decimal notation (e.g., 123.45)\n",
    "- For whole numbers, provide exact integer values\"\"\"\n",
    "    else:\n",
    "        format_instructions = \"\"\"- For numbers, use appropriate precision (integers for whole numbers, decimals for precise values)\n",
    "- For dates, use MM/DD/YYYY format\n",
    "- For gestational age or time periods, use (X weeks, Y days) format (e.g., (0 weeks, 6 days))\"\"\"\n",
    "\n",
    "    system_block = f\"\"\"<|im_start|>system\n",
    "You are a clinical calculation assistant. Extract only the numerical answer or date from the patient case.\n",
    "- Output format: Answer: <number or date only>\n",
    "- Do not explain, justify, or add any text\n",
    "- Do not repeat the question\n",
    "- Use the retrieved examples as guidance only\n",
    "- Follow the exact format of the examples below\n",
    "- {format_instructions}\n",
    "- Ensure your answer falls within reasonable clinical ranges\n",
    "- If unsure, provide your best numerical estimate<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = system_block\n",
    "\n",
    "    # Add retrieved examples as few-shot user/assistant pairs\n",
    "    for i in range(min(n, len(examples))):\n",
    "        ex = examples.iloc[i].to_dict()\n",
    "\n",
    "        ex_note = ex.get(\"Patient Note\", \"\") or \"\"\n",
    "        ex_question = ex.get(\"Question\", \"\") or \"\"\n",
    "        ex_answer = ex.get(\"Ground Truth Answer\", \"\")\n",
    "        if pd.isna(ex_answer) or str(ex_answer).strip() == \"\":\n",
    "            ex_answer_str = \"unknown\"\n",
    "        else:\n",
    "            ex_answer_str = str(ex_answer).strip()\n",
    "\n",
    "        prompt += f\"\"\"<|im_start|>user\n",
    "Patient case:\n",
    "{ex_note}\n",
    "\n",
    "Question: {ex_question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer: {ex_answer_str}<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "    # Target example (the one to answer)\n",
    "    prompt += f\"\"\"<|im_start|>user\n",
    "{entities_section}Patient case:\n",
    "{patient_note}\n",
    "\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# --- RAG Inference over Dataset ---\n",
    "def run_rag_inference(df, model_key, out_file, batch_size=2, use_sampling=True, temperature=0.7):\n",
    "    print(f\"Loading model {model_names[model_key]}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_names[model_key],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    ).eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_names[model_key])\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "\n",
    "    rows = []\n",
    "    batch_prompts = []\n",
    "    batch_meta = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{model_key}-RAG\"):\n",
    "        patient_note = row[\"Patient Note\"]\n",
    "        question = row[\"Question\"]\n",
    "        entities = row.get(\"Relevant Entities\", \"\")\n",
    "        category = row.get(\"Category\", None)\n",
    "        retrieved = retrieve_examples(patient_note, question, k=N_RETRIEVAL)\n",
    "\n",
    "        prompt = rag_prompt(\n",
    "            patient_note,\n",
    "            question,\n",
    "            retrieved,\n",
    "            entities_json=entities,\n",
    "            category=category\n",
    "        )\n",
    "\n",
    "        batch_prompts.append(prompt)\n",
    "        batch_meta.append((idx, row, retrieved))\n",
    "\n",
    "        # Process when batch full\n",
    "        if len(batch_prompts) == batch_size:\n",
    "            rows.extend(_process_batch(batch_prompts, batch_meta, model, tokenizer, model_key, \n",
    "                                     use_sampling=use_sampling, temperature=temperature))\n",
    "            batch_prompts, batch_meta = [], []\n",
    "\n",
    "    # Process remainder\n",
    "    if batch_prompts:\n",
    "        rows.extend(_process_batch(batch_prompts, batch_meta, model, tokenizer, model_key,\n",
    "                                 use_sampling=use_sampling, temperature=temperature))\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_file, index=False)\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def _process_batch(batch_prompts, batch_meta, model, tokenizer, model_key, use_sampling=True, temperature=0.7):\n",
    "    \"\"\"Helper to run a batch of prompts with improved sampling.\"\"\"\n",
    "    results = []\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,   # very low randomness\n",
    "        top_p=0.9,         # nucleus sampling but constrained\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    for (idx, row, retrieved), sample in zip(batch_meta, decoded):\n",
    "        raw_answer = extract_answer_from_output(sample)\n",
    "        category = row.get(\"Category\", None)\n",
    "        parsed_answer = parse_answer(raw_answer, category)\n",
    "        in_range = validate_answer_range(parsed_answer, \n",
    "                                       row.get(\"Lower Limit\"), \n",
    "                                       row.get(\"Upper Limit\"), \n",
    "                                       category)\n",
    "\n",
    "        results.append({\n",
    "            \"index\": idx,\n",
    "            \"id\": row.get(\"Note ID\"),\n",
    "            \"model\": model_key,\n",
    "            \"prompt_type\": \"rag\",\n",
    "            \"category\": row.get(\"Category\"),\n",
    "            \"patient_note\": row[\"Patient Note\"],\n",
    "            \"question\": row[\"Question\"],\n",
    "            \"entities\": row.get(\"Relevant Entities\", \"\"),\n",
    "            \"ground_truth\": row.get(\"Ground Truth Answer\"),\n",
    "            \"lower_limit\": row.get(\"Lower Limit\"),\n",
    "            \"upper_limit\": row.get(\"Upper Limit\"),\n",
    "            \"retrieved_ids\": list(retrieved[\"Note ID\"]) if \"Note ID\" in retrieved else None,\n",
    "            \"raw_output\": sample,\n",
    "            \"extracted_answer\": raw_answer,\n",
    "            \"parsed_answer\": parsed_answer,\n",
    "            \"in_range\": in_range\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- Run for both models with improved parameters ---\n",
    "for model_key in [\"0.6B\", \"1.7B\"]:\n",
    "    out_file = f\"{OUTPUT_DIR}/{model_key}_rag_improved.csv\"\n",
    "    if os.path.exists(out_file):\n",
    "        print(f\"Skipping → {out_file}\")\n",
    "        continue\n",
    "    print(f\"Saving → {out_file}\")\n",
    "    # Use sampling with moderate temperature for better variability\n",
    "    run_rag_inference(test_df, model_key, out_file, \n",
    "                     use_sampling=True, temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Aggregation and Metrics\n",
    "\n",
    "- Loads all output CSVs from `/outputs`.\n",
    "- Uses `in_range` as the correctness indicator (1 if parsed answer within ground-truth range, else 0).\n",
    "- Aggregates accuracy per category for each Model × Method.\n",
    "- Produces a pivoted results table and sample counts for transparency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 CSV files in /outputs\n",
      "\n",
      "Found 10 CSV files:\n",
      "  - /outputs/0.6B_cot_improved.csv\n",
      "  - /outputs/0.6B_few_shot_improved.csv\n",
      "  - /outputs/0.6B_rag_improved.csv\n",
      "  - /outputs/0.6B_zero_shot_improved.csv\n",
      "  - /outputs/0.6B_zero_shot_quantized_lora.csv\n",
      "  - /outputs/1.7B_cot_improved.csv\n",
      "  - /outputs/1.7B_few_shot_improved.csv\n",
      "  - /outputs/1.7B_rag_improved.csv\n",
      "  - /outputs/1.7B_zero_shot_improved.csv\n",
      "  - /outputs/1.7B_zero_shot_quantized_lora.csv\n",
      "Loaded 1047 rows from 0.6B_cot_improved.csv\n",
      "Loaded 1047 rows from 0.6B_few_shot_improved.csv\n",
      "Loaded 1047 rows from 0.6B_rag_improved.csv\n",
      "Loaded 1047 rows from 0.6B_zero_shot_improved.csv\n",
      "Loaded 1047 rows from 0.6B_zero_shot_quantized_lora.csv\n",
      "Loaded 1047 rows from 1.7B_cot_improved.csv\n",
      "Loaded 1047 rows from 1.7B_few_shot_improved.csv\n",
      "Loaded 1047 rows from 1.7B_rag_improved.csv\n",
      "Loaded 1047 rows from 1.7B_zero_shot_improved.csv\n",
      "Loaded 1047 rows from 1.7B_zero_shot_quantized_lora.csv\n",
      "\n",
      "Combined dataset has 10470 rows\n",
      "Columns: ['index', 'id', 'model', 'prompt_type', 'category', 'patient_note', 'question', 'entities', 'ground_truth', 'lower_limit', 'upper_limit', 'raw_output', 'extracted_answer', 'parsed_answer', 'in_range', 'source_file', 'retrieved_ids']\n",
      "Calculated accuracy scores using in_range values.\n",
      "\n",
      "=== ACCURACY RESULTS (Using in_range values) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Model_Prompt</th>\n",
       "      <th>Overall</th>\n",
       "      <th>date</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>dosage</th>\n",
       "      <th>lab</th>\n",
       "      <th>physical</th>\n",
       "      <th>risk</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6B_cot</td>\n",
       "      <td>0.081318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.073394</td>\n",
       "      <td>0.129167</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.0250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6B_few_shot</td>\n",
       "      <td>0.090247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.073394</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.0250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6B_rag</td>\n",
       "      <td>0.166760</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.113150</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6B_zero_shot</td>\n",
       "      <td>0.060922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.076453</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6B_zero_shot_quantized_lora</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.7B_cot</td>\n",
       "      <td>0.137871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>0.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.7B_few_shot</td>\n",
       "      <td>0.106089</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.1125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.7B_rag</td>\n",
       "      <td>0.175890</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.143731</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.7B_zero_shot</td>\n",
       "      <td>0.123351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.158333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.7B_zero_shot_quantized_lora</td>\n",
       "      <td>0.045364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055046</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>0.0375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category                   Model_Prompt   Overall      date  diagnosis  \\\n",
       "0                              0.6B_cot  0.081318  0.000000   0.233333   \n",
       "1                         0.6B_few_shot  0.090247  0.000000   0.266667   \n",
       "2                              0.6B_rag  0.166760  0.033333   0.383333   \n",
       "3                        0.6B_zero_shot  0.060922  0.000000   0.150000   \n",
       "4         0.6B_zero_shot_quantized_lora  0.010398  0.000000   0.000000   \n",
       "5                              1.7B_cot  0.137871  0.000000   0.283333   \n",
       "6                         1.7B_few_shot  0.106089  0.016667   0.183333   \n",
       "7                              1.7B_rag  0.175890  0.033333   0.283333   \n",
       "8                        1.7B_zero_shot  0.123351  0.000000   0.233333   \n",
       "9         1.7B_zero_shot_quantized_lora  0.045364  0.000000   0.066667   \n",
       "\n",
       "Category  dosage       lab  physical      risk  severity  \n",
       "0          0.075  0.073394  0.129167  0.033333    0.0250  \n",
       "1          0.075  0.073394  0.158333  0.033333    0.0250  \n",
       "2          0.075  0.113150  0.437500  0.062500    0.0625  \n",
       "3          0.075  0.076453  0.091667  0.020833    0.0125  \n",
       "4          0.025  0.006116  0.041667  0.000000    0.0000  \n",
       "5          0.075  0.119266  0.216667  0.120833    0.1500  \n",
       "6          0.050  0.146789  0.133333  0.100000    0.1125  \n",
       "7          0.075  0.143731  0.462500  0.108333    0.1250  \n",
       "8          0.075  0.146789  0.158333  0.100000    0.1500  \n",
       "9          0.000  0.055046  0.079167  0.079167    0.0375  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAMPLE COUNTS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Category</th>\n",
       "      <th>Model_Prompt</th>\n",
       "      <th>Overall</th>\n",
       "      <th>date</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>dosage</th>\n",
       "      <th>lab</th>\n",
       "      <th>physical</th>\n",
       "      <th>risk</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6B_cot</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6B_few_shot</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6B_rag</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6B_zero_shot</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6B_zero_shot_quantized_lora</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.7B_cot</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.7B_few_shot</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.7B_rag</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.7B_zero_shot</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.7B_zero_shot_quantized_lora</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category                   Model_Prompt  Overall  date  diagnosis  dosage  \\\n",
       "0                              0.6B_cot   1047.0  60.0       60.0    40.0   \n",
       "1                         0.6B_few_shot   1047.0  60.0       60.0    40.0   \n",
       "2                              0.6B_rag   1047.0  60.0       60.0    40.0   \n",
       "3                        0.6B_zero_shot   1047.0  60.0       60.0    40.0   \n",
       "4         0.6B_zero_shot_quantized_lora   1047.0  60.0       60.0    40.0   \n",
       "5                              1.7B_cot   1047.0  60.0       60.0    40.0   \n",
       "6                         1.7B_few_shot   1047.0  60.0       60.0    40.0   \n",
       "7                              1.7B_rag   1047.0  60.0       60.0    40.0   \n",
       "8                        1.7B_zero_shot   1047.0  60.0       60.0    40.0   \n",
       "9         1.7B_zero_shot_quantized_lora   1047.0  60.0       60.0    40.0   \n",
       "\n",
       "Category    lab  physical   risk  severity  \n",
       "0         327.0     240.0  240.0      80.0  \n",
       "1         327.0     240.0  240.0      80.0  \n",
       "2         327.0     240.0  240.0      80.0  \n",
       "3         327.0     240.0  240.0      80.0  \n",
       "4         327.0     240.0  240.0      80.0  \n",
       "5         327.0     240.0  240.0      80.0  \n",
       "6         327.0     240.0  240.0      80.0  \n",
       "7         327.0     240.0  240.0      80.0  \n",
       "8         327.0     240.0  240.0      80.0  \n",
       "9         327.0     240.0  240.0      80.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY STATISTICS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Prompt</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Total Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6B_cot</td>\n",
       "      <td>0.078319</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6B_few_shot</td>\n",
       "      <td>0.086915</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6B_rag</td>\n",
       "      <td>0.181471</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6B_zero_shot</td>\n",
       "      <td>0.062082</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6B_zero_shot_quantized_lora</td>\n",
       "      <td>0.012416</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.7B_cot</td>\n",
       "      <td>0.145177</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.7B_few_shot</td>\n",
       "      <td>0.121299</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.7B_rag</td>\n",
       "      <td>0.206304</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.7B_zero_shot</td>\n",
       "      <td>0.132760</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.7B_zero_shot_quantized_lora</td>\n",
       "      <td>0.060172</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model_Prompt  Overall Accuracy  Total Samples\n",
       "0                       0.6B_cot          0.078319           1047\n",
       "1                  0.6B_few_shot          0.086915           1047\n",
       "2                       0.6B_rag          0.181471           1047\n",
       "3                 0.6B_zero_shot          0.062082           1047\n",
       "4  0.6B_zero_shot_quantized_lora          0.012416           1047\n",
       "5                       1.7B_cot          0.145177           1047\n",
       "6                  1.7B_few_shot          0.121299           1047\n",
       "7                       1.7B_rag          0.206304           1047\n",
       "8                 1.7B_zero_shot          0.132760           1047\n",
       "9  1.7B_zero_shot_quantized_lora          0.060172           1047"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to find all CSV files in outputs directory\n",
    "def find_csv_files():\n",
    "    \"\"\"Find all CSV files in the outputs directory.\"\"\"\n",
    "    output_dir = '/outputs'\n",
    "    if os.path.exists(output_dir):\n",
    "        pattern = os.path.join(output_dir, \"*.csv\")\n",
    "        csv_files = sorted(glob.glob(pattern))\n",
    "        print(f\"Found {len(csv_files)} CSV files in {output_dir}\")\n",
    "        return csv_files\n",
    "    else:\n",
    "        print(f\"Output directory {output_dir} does not exist\")\n",
    "        return []\n",
    "\n",
    "# Find all CSV files\n",
    "csv_files = find_csv_files()\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found. Please ensure your inference has been run and files are saved.\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(csv_files)} CSV files:\")\n",
    "    for file in csv_files:\n",
    "        print(f\"  - {file}\")\n",
    "\n",
    "    # Read and combine all CSV files\n",
    "    all_data = []\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # Add source file information\n",
    "            df['source_file'] = os.path.basename(csv_file)\n",
    "            all_data.append(df)\n",
    "            print(f\"Loaded {len(df)} rows from {os.path.basename(csv_file)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "\n",
    "    if all_data:\n",
    "        # Combine all data\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\nCombined dataset has {len(combined_df)} rows\")\n",
    "        \n",
    "        # Display column names to verify structure\n",
    "        print(f\"Columns: {list(combined_df.columns)}\")\n",
    "        \n",
    "        # Check if in_range column exists\n",
    "        if 'in_range' not in combined_df.columns:\n",
    "            print(\"Warning: 'in_range' column not found. Available columns:\", list(combined_df.columns))\n",
    "        else:\n",
    "            # Calculate accuracy using in_range values\n",
    "            evaluation_results = {}\n",
    "            \n",
    "            for _, row in combined_df.iterrows():\n",
    "                file_name = row['source_file']\n",
    "                model = row['model'] if 'model' in row else 'unknown'\n",
    "                prompt_type = row['prompt_type'] if 'prompt_type' in row else 'unknown'\n",
    "                category = row['category'] if 'category' in row else 'unknown'\n",
    "                \n",
    "                # Use in_range as accuracy (1 if True, 0 if False)\n",
    "                accuracy = 1 if row['in_range'] else 0\n",
    "                \n",
    "                # Create unique key combining model and prompt_type\n",
    "                key = f\"{model}_{prompt_type}\"\n",
    "                \n",
    "                if key not in evaluation_results:\n",
    "                    evaluation_results[key] = {}\n",
    "                \n",
    "                if category not in evaluation_results[key]:\n",
    "                    evaluation_results[key][category] = {'accuracy': []}\n",
    "                \n",
    "                evaluation_results[key][category]['accuracy'].append(accuracy)\n",
    "            \n",
    "            print(\"Calculated accuracy scores using in_range values.\")\n",
    "            \n",
    "            # Aggregate results\n",
    "            aggregated_results = []\n",
    "            \n",
    "            for key, categories in evaluation_results.items():\n",
    "                for category, scores in categories.items():\n",
    "                    # Calculate average accuracy\n",
    "                    avg_accuracy = sum(scores['accuracy']) / len(scores['accuracy']) if scores['accuracy'] else 0\n",
    "                    \n",
    "                    aggregated_results.append({\n",
    "                        'Model_Prompt': key,\n",
    "                        'Category': category,\n",
    "                        'Average Accuracy': avg_accuracy,\n",
    "                        'Sample Count': len(scores['accuracy'])\n",
    "                    })\n",
    "            \n",
    "            results_df = pd.DataFrame(aggregated_results)\n",
    "            \n",
    "            # Calculate overall averages for each model-prompt combination\n",
    "            overall_results = results_df.groupby('Model_Prompt')[['Average Accuracy']].mean().reset_index()\n",
    "            overall_results['Category'] = 'Overall'\n",
    "            overall_results['Sample Count'] = results_df.groupby('Model_Prompt')['Sample Count'].sum().values\n",
    "            \n",
    "            # Append overall results to the aggregated results\n",
    "            aggregated_results_with_overall = pd.concat([results_df, overall_results], ignore_index=True)\n",
    "            \n",
    "            # Pivot the table to have categories as columns, showing average accuracy\n",
    "            pivot_results_df = aggregated_results_with_overall.pivot_table(\n",
    "                index='Model_Prompt',\n",
    "                columns='Category',\n",
    "                values='Average Accuracy',\n",
    "                fill_value=0\n",
    "            ).reset_index()\n",
    "            \n",
    "            print(\"\\n=== ACCURACY RESULTS (Using in_range values) ===\")\n",
    "            display(pivot_results_df)\n",
    "            \n",
    "            # Also show sample counts\n",
    "            sample_counts_df = aggregated_results_with_overall.pivot_table(\n",
    "                index='Model_Prompt',\n",
    "                columns='Category',\n",
    "                values='Sample Count',\n",
    "                fill_value=0\n",
    "            ).reset_index()\n",
    "            \n",
    "            print(\"\\n=== SAMPLE COUNTS ===\")\n",
    "            display(sample_counts_df)\n",
    "            \n",
    "            # Summary statistics\n",
    "            print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "            summary_stats = []\n",
    "            for key in evaluation_results.keys():\n",
    "                overall_acc = []\n",
    "                for category in evaluation_results[key].values():\n",
    "                    overall_acc.extend(category['accuracy'])\n",
    "                \n",
    "                summary_stats.append({\n",
    "                    'Model_Prompt': key,\n",
    "                    'Overall Accuracy': sum(overall_acc) / len(overall_acc) if overall_acc else 0,\n",
    "                    'Total Samples': len(overall_acc)\n",
    "                })\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_stats)\n",
    "            display(summary_df)\n",
    "    else:\n",
    "        print(\"No data could be loaded from CSV files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Results\n",
    "\n",
    "This block writes the aggregated evaluation artifacts to repository CSVs for downstream reporting and the README table:\n",
    "- `aggregated_results.csv`: per-run category rows\n",
    "- `overall_results.csv`: overall accuracy by Model × Method\n",
    "- `pivot_results.csv`: wide table for quick viewing\n",
    "- `sample_counts.csv`: counts by category\n",
    "- `summary_statistics.csv`: global rollups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All evaluation results saved as CSV files:\n",
      " - aggregated_results.csv\n",
      " - overall_results.csv\n",
      " - pivot_results.csv\n",
      " - sample_counts.csv\n",
      " - summary_statistics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save aggregated results to CSV\n",
    "results_df.to_csv(\"aggregated_results.csv\", index=False)\n",
    "overall_results.to_csv(\"overall_results.csv\", index=False)\n",
    "pivot_results_df.to_csv(\"pivot_results.csv\", index=False)\n",
    "sample_counts_df.to_csv(\"sample_counts.csv\", index=False)\n",
    "summary_df.to_csv(\"summary_statistics.csv\", index=False)\n",
    "\n",
    "print(\"All evaluation results saved as CSV files:\")\n",
    "print(\" - aggregated_results.csv\")\n",
    "print(\" - overall_results.csv\")\n",
    "print(\" - pivot_results.csv\")\n",
    "print(\" - sample_counts.csv\")\n",
    "print(\" - summary_statistics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2193aa8aa26f4f6098638667ff4c23e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23524eeb480b48dca2d7be051d60ef5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28b118d7850d41d7a1ad8d473adbb3be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bbc6e995c1f4f63be50f5e9dad6e376": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2def2041cb114c1db533ead31a234748": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4daff3b9a2af4cdcb26300d230ecfc94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd985c75884a4ba5992726c376a0f0b8",
      "placeholder": "​",
      "style": "IPY_MODEL_23524eeb480b48dca2d7be051d60ef5a",
      "value": " 10053/10053 [00:33&lt;00:00, 758.77 examples/s]"
     }
    },
    "5a186ceda6324233860986498222e2ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28b118d7850d41d7a1ad8d473adbb3be",
      "max": 1047,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0f347bff23b40ae900077846b5441e7",
      "value": 0
     }
    },
    "6ef7346f78a444a282edf65a4c2479e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae1bc63b4e8c42b896f6c9999bcde18b",
      "max": 10053,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2def2041cb114c1db533ead31a234748",
      "value": 10053
     }
    },
    "7217115a6f304797919d81a23968b486": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "801b36af9e7045e596f8b719cef3e3f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9186461e305f4e80ba48cbd1897d7688": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d20d912668245d483fb35e33a2d9555": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccac27e3c2bf45e7af1e81fe577e32ed",
      "placeholder": "​",
      "style": "IPY_MODEL_7217115a6f304797919d81a23968b486",
      "value": "Map: 100%"
     }
    },
    "acae790e32c74a82bd4c8b05f463e1f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bebd4dc4632d48b784a1f2910a533114",
      "placeholder": "​",
      "style": "IPY_MODEL_9186461e305f4e80ba48cbd1897d7688",
      "value": " 0/1047 [00:01&lt;?, ?it/s]"
     }
    },
    "ae1bc63b4e8c42b896f6c9999bcde18b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0f347bff23b40ae900077846b5441e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bebd4dc4632d48b784a1f2910a533114": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccac27e3c2bf45e7af1e81fe577e32ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd985c75884a4ba5992726c376a0f0b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd08890a2bcc4bea9e1d8f295ad48c9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2193aa8aa26f4f6098638667ff4c23e2",
      "placeholder": "​",
      "style": "IPY_MODEL_fce5a57cb99049fa9fe98c4c9cf94c15",
      "value": "0.6B-few_shot:   0%"
     }
    },
    "e4862d4872a1492088c2a3a0d943ae74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d20d912668245d483fb35e33a2d9555",
       "IPY_MODEL_6ef7346f78a444a282edf65a4c2479e6",
       "IPY_MODEL_4daff3b9a2af4cdcb26300d230ecfc94"
      ],
      "layout": "IPY_MODEL_801b36af9e7045e596f8b719cef3e3f0"
     }
    },
    "e6e7ca43d838432bb6d10b13d5a947fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd08890a2bcc4bea9e1d8f295ad48c9d",
       "IPY_MODEL_5a186ceda6324233860986498222e2ae",
       "IPY_MODEL_acae790e32c74a82bd4c8b05f463e1f9"
      ],
      "layout": "IPY_MODEL_2bbc6e995c1f4f63be50f5e9dad6e376"
     }
    },
    "fce5a57cb99049fa9fe98c4c9cf94c15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
