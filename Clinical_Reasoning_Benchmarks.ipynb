{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Reasoning Model Benchmarking and Fine-Tuning (MedCalc-Bench)\n",
    "\n",
    "This notebook benchmarks and fine-tunes Qwen3 models on MedCalc-Bench clinical calculation tasks. It covers: prompt engineering (zero-shot, few-shot, CoT), optional advanced methods, parameter-efficient fine-tuning (LoRA/QLoRA), and category-wise evaluation.\n",
    "\n",
    "**Use this on Google Colab Free Tier or locally. All steps are reproducible and parameter choices are explained.**\n",
    "\n",
    "## Contents\n",
    "- **Setup and data loading**\n",
    "- **Prompt engineering**: Zero-shot, Few-shot, CoT (+ optional advanced)\n",
    "- **Fine-tuning with LoRA/QLoRA (PEFT)**\n",
    "- **Inference and evaluation**\n",
    "- **Results visualization and interpretation**\n",
    "\n",
    "> **Tip**: If you are in Colab, switch to T4 GPU in Runtime > Change runtime type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Reproduction Instructions\n",
    "\n",
    "### Google Colab Setup (Recommended)\n",
    "1. **Open in Colab**: Click \"Open in Colab\" button or upload to Google Colab\n",
    "2. **Enable GPU**: Runtime → Change runtime type → T4 GPU\n",
    "3. **Run setup cell**: Execute the installation cell below\n",
    "4. **Expected runtime**: ~2-3 hours for full evaluation\n",
    "\n",
    "### Local Setup (Alternative)\n",
    "1. **Python environment**: Python 3.8+ with pip/conda\n",
    "2. **GPU requirements**: CUDA-compatible GPU with 8GB+ VRAM\n",
    "3. **Dependencies**: Install packages from setup cell below\n",
    "4. **Data**: Clone repository with all files\n",
    "\n",
    "### Hardware Requirements\n",
    "- **Minimum**: T4 GPU (Colab Free) - 16GB VRAM\n",
    "- **Recommended**: A10/V100 GPU - 24GB+ VRAM for faster training\n",
    "- **CPU**: 4+ cores, 16GB+ RAM\n",
    "- **Storage**: 10GB+ free space for models and results\n",
    "\n",
    "### Expected Outputs\n",
    "- **Model checkpoints**: Saved in `qwen_lora_*/` directories\n",
    "- **Results**: CSV files in `outputs/` directory  \n",
    "- **Visualizations**: PNG files in `images/` directory\n",
    "- **Total runtime**: 2-4 hours depending on hardware\n",
    "\n",
    "**Documentation Approach:**\n",
    "Parameters and decisions are documented inline before each major block to ensure reproducibility and understanding of design choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-interactive install for Colab/local\n",
    "# - -q keeps output terse in notebooks\n",
    "# - Pin transformers >=4.43.0 for Qwen3 compatibility and latest generate features\n",
    "# - bitsandbytes enables 8-bit/4-bit loading for memory savings\n",
    "# - accelerate/peft support device placement and parameter-efficient fine-tuning\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q \"transformers>=4.43.0\" accelerate peft bitsandbytes datasets evaluate scikit-learn seaborn matplotlib pandas numpy\n",
    "!pip install -q einops xformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Structure\n",
    "\n",
    "**Dataset:** MedCalc-Bench with fields: `Patient Note`, `Question`, `Category`, `Relevant Entities` (JSON), `Ground Truth Answer`, `Ground Truth Explanation`.\n",
    "\n",
    "**Data Sources:**\n",
    "- Train file: `dataset/train_data.csv`\n",
    "- Test file: `dataset/test_data.csv`\n",
    "\n",
    "> **Note**: If you are only reproducing results, skip to the Results section; otherwise load the dataset for fine-tuning/inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "id": "rRmDapWqLQ3G",
    "outputId": "8bfd35f2-114f-44ac-a255-6448c9b8e703"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load MedCalc-Bench directly from Hugging Face\n",
    "dataset = load_dataset(\"ncbi/MedCalc-Bench-v1.0\")\n",
    "\n",
    "# Split into train/test\n",
    "train_df = dataset[\"train\"].to_pandas()\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "# Preview\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "#train_df.head()\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore distribution of output types to guide prompt formatting and parsing logic\n",
    "# - 'decimal' vs 'integer' steer numeric casting\n",
    "# - 'date' activates MM/DD/YYYY and gestational age tuple handling\n",
    "output_types = train_df[\"Output Type\"].unique()\n",
    "print(\"Unique Output Types:\", output_types)\n",
    "\n",
    "# Show a representative example per type to sanity-check fields and ranges\n",
    "for ot in output_types:\n",
    "    print(f\"\\nSample for Output Type: {ot}\")\n",
    "    print(train_df[train_df[\"Output Type\"] == ot].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Models\n",
    "\n",
    "**Model Selection Rationale:**\n",
    "We evaluate two Qwen3 sizes for compute/accuracy tradeoffs:\n",
    "\n",
    "- **Qwen3-0.6B**: Fast inference, low VRAM requirements, strong baseline with prompt engineering\n",
    "- **Qwen3-1.7B**: Higher capacity, better reasoning capabilities, still Colab-compatible with optimizations\n",
    "\n",
    "**Why These Models:**\n",
    "- Cover the speed/accuracy trade-off spectrum\n",
    "- Both fit comfortably on common GPUs (T4/A10) with proper optimization\n",
    "- Enable fair comparison of prompt engineering vs. fine-tuning approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tT0tsDpkiqkq"
   },
   "outputs": [],
   "source": [
    "# Model names mapped to Hugging Face hub identifiers\n",
    "# Rationale:\n",
    "# - Two sizes cover speed/accuracy trade-offs and fit on common GPUs (T4/A10)\n",
    "# - Keeping a dict enables uniform loops for loading/evaluation\n",
    "model_names = {\n",
    "    \"0.6B\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"1.7B\": \"Qwen/Qwen3-1.7B\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Tokenizer Loading (Qwen3-0.6B, Qwen3-1.7B)\n",
    "\n",
    "- Left-padding/truncation for decoder-only models to align attention masks.\n",
    "- bfloat16 weights if available; `device_map='auto'` to distribute on GPU.\n",
    "- Loads both models to compare prompt methods fairly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVfGw7ebLQ3U",
    "outputId": "68d6cc6d-f2f8-43ab-b657-fc171a1c20f6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load models and tokenizers with settings optimized for decoder-only Qwen3\n",
    "# Parameter choices and reasoning:\n",
    "# - padding_side = \"left\": Enables efficient batching for decoder-only LMs where attention\n",
    "#   depends on position from the right; avoids shifting attention masks across batch items.\n",
    "# - truncation_side = \"left\": Keeps the most recent and typically most relevant context\n",
    "#   when sequences exceed max length (useful for long patient notes).\n",
    "# - torch_dtype = torch.bfloat16: Cuts memory roughly in half compared to fp32 while\n",
    "#   maintaining numerical stability on modern GPUs (A100, T4 w/ emulation). Falls back\n",
    "#   to fp16 automatically at runtime if bfloat16 is unsupported.\n",
    "# - device_map = \"auto\": Automatically places weights on available GPU(s) to prevent OOM\n",
    "#   and leverages VRAM without manual layer placement.\n",
    "# - trust_remote_code = True: Required for some community models (like Qwen) that ship\n",
    "#   custom modeling code; vetted upstream but should be used only with trusted repos.\n",
    "models, tokenizers = {}, {}\n",
    "for key, name in model_names.items():\n",
    "    print(f\"Loading {name}...\")\n",
    "    tokenizers[key] = AutoTokenizer.from_pretrained(name)\n",
    "    tokenizers[key].padding_side = \"left\"   # left-padding for batched decoder-only generation\n",
    "    tokenizers[key].truncation_side = \"left\"  # preserve most recent context under truncation\n",
    "    models[key] = AutoModelForCausalLM.from_pretrained(\n",
    "        name,\n",
    "        torch_dtype=torch.bfloat16,  # memory-efficient weights with stable numerics\n",
    "        device_map=\"auto\",          # auto-shard across GPU(s) if present\n",
    "        trust_remote_code=True       # allow model-specific implementations\n",
    "    )\n",
    "\n",
    "# Quick check of architectures for sanity (helps ensure correct variant is loaded)\n",
    "print(\"Models loaded:\")\n",
    "for key in models:\n",
    "    print(key, models[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightweight Generator Wrapper\n",
    "\n",
    "- Encodes prompt and generates a short completion.\n",
    "- Conservative decoding defaults keep outputs concise and reproducible across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWuWp5xKLQ3W"
   },
   "outputs": [],
   "source": [
    "def generate_answer(prompt, model, tokenizer, max_length=32):\n",
    "    \"\"\"Lightweight wrapper around generate for quick single-prompt checks.\n",
    "    - Keeps temperature low to reduce variance while allowing minor exploration.\n",
    "    - Uses left-padding defaults already set on tokenizer.\n",
    "    - max_length defaults to 32 to encourage concise answers (e.g., numbers/dates).\n",
    "    \"\"\"\n",
    "    # Tokenize to tensors and place on GPU for faster inference\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Sampling choices:\n",
    "    # - do_sample=True with temperature=0.3: allows slight exploration while keeping outputs stable\n",
    "    # - top_p=0.9: nucleus sampling trims tail probabilities to avoid degenerate loops\n",
    "    # - eos_token_id/pad_token_id: ensure proper early stopping and consistent padding behavior\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_length,   # cap new tokens to keep outputs short and on-task\n",
    "        do_sample=True,\n",
    "        temperature=0.3,             # low randomness for reproducible numeric outputs\n",
    "        top_p=0.9,                   # constrain to high-probability tokens\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Convert token IDs back to string, skipping any special tokens that may appear\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Extraction, Parsing, and Validation\n",
    "\n",
    "- Robust `Answer:` extraction from free-form generations.\n",
    "- Category-aware parsing (numeric vs. date vs. gestational age tuples).\n",
    "- Range validation against provided lower/upper limits; this drives accuracy metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MNwlI-MLQ3Z"
   },
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Answer Extraction and Validation Functions\n",
    "# -------------------\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def extract_answer_from_output(output_text):\n",
    "    \"\"\"Extract the last occurrence of 'Answer:' from model output.\n",
    "    Rationale: models may emit multiple 'Answer:' lines (e.g., retries or CoT).\n",
    "    Choosing the last occurrence biases toward the final corrected answer.\n",
    "    \"\"\"\n",
    "    # Find all occurrences of \"Answer:\"\n",
    "    answer_matches = re.findall(r'Answer:\\s*(.+?)(?:\\n|$)', output_text, re.IGNORECASE)\n",
    "    if answer_matches:\n",
    "        last_answer = answer_matches[-1].strip()\n",
    "        # Remove any additional text after a newline just in case\n",
    "        last_answer = last_answer.split('\\n')[0].strip()\n",
    "        return last_answer\n",
    "    \n",
    "    return \"N/A\"\n",
    "\n",
    "def parse_answer(answer_str, category):\n",
    "    \"\"\"Parse answer string based on category with stricter rules.\"\"\"\n",
    "    if pd.isna(answer_str) or str(answer_str).lower() == \"unknown\":\n",
    "        return None\n",
    "\n",
    "    text = str(answer_str).strip()\n",
    "\n",
    "    try:\n",
    "        if category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "            # Find the first valid number (int or decimal)\n",
    "            num_match = re.search(r'\\d+(?:\\.\\d+)?', text)\n",
    "            if num_match:\n",
    "                num_str = num_match.group(0)\n",
    "                if '.' in num_str:\n",
    "                    return float(num_str)\n",
    "                else:\n",
    "                    return int(float(num_str))\n",
    "            return None\n",
    "\n",
    "        elif category == \"date\":\n",
    "            # First, check for (weeks, days) pattern — this should take priority and return early\n",
    "            # Match (X weeks, Y days) with optional single/double quotes around each part\n",
    "            weeks_days_match = re.search(\n",
    "                r'''\n",
    "                \\(                          # Opening parenthesis\n",
    "                \\s*                         # Optional whitespace\n",
    "                [\\'\"]?                      # Optional opening quote (single or double)\n",
    "                (\\d+)                       # CAPTURE: weeks number\n",
    "                \\s*                         # Optional whitespace\n",
    "                weeks?                      # \"week\" or \"weeks\"\n",
    "                [\\'\"]?                      # Optional closing quote\n",
    "                \\s*                         # Optional whitespace\n",
    "                (?:,|\\s+and\\s+|\\s+)         # Separator: comma, \"and\", or whitespace\n",
    "                \\s*                         # Optional whitespace\n",
    "                [\\'\"]?                      # Optional opening quote for days\n",
    "                (\\d+)                       # CAPTURE: days number\n",
    "                \\s*                         # Optional whitespace\n",
    "                days?                       # \"day\" or \"days\"\n",
    "                [\\'\"]?                      # Optional closing quote\n",
    "                \\s*                         # Optional whitespace\n",
    "                \\)                          # Closing parenthesis\n",
    "                ''',\n",
    "                text,\n",
    "                re.IGNORECASE | re.VERBOSE\n",
    "            )\n",
    "            if weeks_days_match:\n",
    "                weeks = int(weeks_days_match.group(1))\n",
    "                days = int(weeks_days_match.group(2))\n",
    "                return (weeks, days)\n",
    "        \n",
    "            # If no weeks/days found, try to extract a clean date string\n",
    "            # Look for the first valid date format (xx/xx/xxxx or similar)\n",
    "            date_clip = re.search(r'(\\d{1,2}/\\d{1,2}/\\d{4})|(\\d{4}-\\d{1,2}-\\d{1,2})', text)\n",
    "            if date_clip:\n",
    "                date_str = date_clip.group(0)  # Take the first match\n",
    "                # Now parse this clean date string\n",
    "                date_formats = [\n",
    "                    \"%m/%d/%Y\", \"%m/%d/%y\",\n",
    "                    \"%Y-%m-%d\", \"%Y/%m/%d\",\n",
    "                    \"%d/%m/%Y\", \"%d/%m/%y\",\n",
    "                    \"%Y-%m\", \"%Y\", \"%m/%Y\", \"%Y/%m\"\n",
    "                ]\n",
    "                for fmt in date_formats:\n",
    "                    try:\n",
    "                        parsed_date = datetime.strptime(date_str, fmt)\n",
    "                        return parsed_date.strftime(\"%m/%d/%Y\")\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        \n",
    "            # Fallback: extract first 4-digit year\n",
    "            year_match = re.search(r'\\b(\\d{4})\\b', text)\n",
    "            if year_match:\n",
    "                return f\"01/01/{year_match.group(1)}\"\n",
    "        \n",
    "            return None\n",
    "\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def validate_answer_range(answer, lower_limit, upper_limit, category):\n",
    "    \"\"\"Check if answer falls within the expected range.\n",
    "    Rationale:\n",
    "    - Returns False on missing inputs to keep the metric conservative.\n",
    "    - Numeric categories: inclusive range check after float casting to tolerate\n",
    "      integer/float formatting differences.\n",
    "    - Date category: exact tuple match for (weeks, days); otherwise inclusive\n",
    "      range check on standardized MM/DD/YYYY strings.\n",
    "    \"\"\"\n",
    "    if answer is None or pd.isna(lower_limit) or pd.isna(upper_limit):\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        if category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "            answer_val = float(answer)\n",
    "            lower_val = float(lower_limit)\n",
    "            upper_val = float(upper_limit)\n",
    "            return lower_val <= answer_val <= upper_val\n",
    "        elif category == \"date\":\n",
    "            # Handle (weeks, days) format - exact match required\n",
    "            if isinstance(answer, tuple) and len(answer) == 2:\n",
    "                # This is a (weeks, days) format - check for exact match\n",
    "                try:\n",
    "                    # Parse the ground truth to see if it's also in (weeks, days) format\n",
    "                    gt_weeks_days_match = re.search(r'\\((\\d+)\\s*weeks?[,\\s]*[\\'\"]?\\s*(\\d+)\\s*days?[\\'\"]?\\)', str(lower_limit), re.IGNORECASE)\n",
    "                    if gt_weeks_days_match:\n",
    "                        gt_weeks = int(gt_weeks_days_match.group(1))\n",
    "                        gt_days = int(gt_weeks_days_match.group(2))\n",
    "                        return answer == (gt_weeks, gt_days)\n",
    "                    else:\n",
    "                        # If ground truth is not in (weeks, days) format, no match\n",
    "                        return False\n",
    "                except (ValueError, TypeError):\n",
    "                    return False\n",
    "            \n",
    "            # For regular dates, we need to parse them\n",
    "            try:\n",
    "                answer_date = datetime.strptime(str(answer), \"%m/%d/%Y\")\n",
    "                lower_date = datetime.strptime(str(lower_limit), \"%m/%d/%Y\")\n",
    "                upper_date = datetime.strptime(str(upper_limit), \"%m/%d/%Y\")\n",
    "                return lower_date <= answer_date <= upper_date\n",
    "            except ValueError:\n",
    "                return False\n",
    "    except (ValueError, TypeError):\n",
    "        return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "# -------------------\n",
    "# Enhanced Zero-Shot Prompt with Better Date Handling\n",
    "# -------------------\n",
    "def zero_shot_prompt(patient_note, question, entities_json=None, category=None):\n",
    "    entities_section = f\"Relevant Entities: {entities_json}\\n\\n\" if entities_json else \"\"\n",
    "    \n",
    "    # Determine format instructions based on category\n",
    "    if category == \"date\":\n",
    "        format_instructions = \"\"\"- For dates, use MM/DD/YYYY format (e.g., 08/31/2023)\n",
    "- If only year is available, use 01/01/YYYY\n",
    "- If year and month are available, use MM/01/YYYY\n",
    "- For gestational age or time periods, use (X weeks, Y days) format (e.g., (0 weeks, 6 days))\n",
    "- Extract the most specific date information available\"\"\"\n",
    "    elif category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "        format_instructions = \"\"\"- For numeric values, provide precise values with appropriate decimal places\n",
    "- Round to 2-3 decimal places unless more precision is needed\n",
    "- Use standard decimal notation (e.g., 123.45)\n",
    "- For whole numbers, provide exact integer values\"\"\"\n",
    "    else:\n",
    "        format_instructions = \"\"\"- For numbers, use appropriate precision (integers for whole numbers, decimals for precise values)\n",
    "- For dates, use MM/DD/YYYY format\"\"\"\n",
    "    \n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a clinical calculation assistant. Extract only the numerical answer or date from the patient case.\n",
    "- Output format: Answer: <number or date only>\n",
    "- Do not explain, justify, or add any text\n",
    "- Do not repeat the question\n",
    "- {format_instructions}\n",
    "- If unsure, provide your best numerical estimate\n",
    "- Ensure your answer falls within reasonable clinical ranges<|im_end|>\n",
    "<|im_start|>user\n",
    "{entities_section}Patient case:\n",
    "{patient_note}\n",
    "\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer:\"\"\"\n",
    "\n",
    "# -------------------\n",
    "# Enhanced Few-Shot Prompt with Better Examples\n",
    "# -------------------\n",
    "def few_shot_prompt(patient_note, question, examples, n, entities_json=None, category=None):\n",
    "    # Determine format instructions based on category\n",
    "    if category == \"date\":\n",
    "        format_instructions = \"\"\"- For dates, use MM/DD/YYYY format (e.g., 08/31/2023)\n",
    "- If only year is available, use 01/01/YYYY\n",
    "- If year and month are available, use MM/01/YYYY\n",
    "- For gestational age or time periods, use (X weeks, Y days) format (e.g., (0 weeks, 6 days))\n",
    "- Extract the most specific date information available\"\"\"\n",
    "    elif category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "        format_instructions = \"\"\"- For numeric values, provide precise values with appropriate decimal places\n",
    "- Round to 2-3 decimal places unless more precision is needed\n",
    "- Use standard decimal notation (e.g., 123.45)\n",
    "- For whole numbers, provide exact integer values\"\"\"\n",
    "    else:\n",
    "        format_instructions = \"\"\"- For numbers, use appropriate precision (integers for whole numbers, decimals for precise values)\n",
    "- For dates, use MM/DD/YYYY format\"\"\"\n",
    "    \n",
    "    prompt_text = f\"\"\"<|im_start|>system\n",
    "You are a clinical calculation assistant. Extract only the numerical answer or date from the patient case.\n",
    "- Output format: Answer: <number or date only>\n",
    "- Do not explain, justify, or add any text\n",
    "- Do not repeat the question\n",
    "- {format_instructions}\n",
    "- Follow the exact format of the examples below\n",
    "- Ensure your answer falls within reasonable clinical ranges<|im_end|>\n",
    "\"\"\"\n",
    "    \n",
    "    for i in range(min(n, len(examples))):\n",
    "        ex = examples.iloc[i]\n",
    "        # Format the ground truth answer properly\n",
    "        gt_answer = ex['Ground Truth Answer']\n",
    "        if pd.isna(gt_answer) or str(gt_answer).strip() == \"\":\n",
    "            gt_answer_str = \"unknown\"\n",
    "        else:\n",
    "            gt_answer_str = str(gt_answer).strip()\n",
    "        \n",
    "        prompt_text += f\"\"\"<|im_start|>user\n",
    "Patient case:\n",
    "{ex['Patient Note']}\n",
    "\n",
    "Question: {ex['Question']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer: {gt_answer_str}<|im_end|>\n",
    "\"\"\"\n",
    "    \n",
    "    entities_section = f\"Relevant Entities: {entities_json}\\n\\n\" if entities_json else \"\"\n",
    "    prompt_text += f\"\"\"<|im_start|>user\n",
    "{entities_section}Patient case:\n",
    "{patient_note}\n",
    "\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer:\"\"\"\n",
    "    return prompt_text\n",
    "\n",
    "# -------------------\n",
    "# Enhanced Chain-of-Thought (CoT) Prompt with Better Reasoning\n",
    "# -------------------\n",
    "def CoT(patient_note, question, entities_json=None, category=None):\n",
    "    entities_section = f\"Relevant Entities: {entities_json}\\n\\n\" if entities_json else \"\"\n",
    "    \n",
    "    # Determine format instructions based on category\n",
    "    if category == \"date\":\n",
    "        format_instructions = \"\"\"- For dates, use MM/DD/YYYY format (e.g., 08/31/2023)\n",
    "- If only year is available, use 01/01/YYYY\n",
    "- If year and month are available, use MM/01/YYYY\n",
    "- For gestational age or time periods, use (X weeks, Y days) format (e.g., (0 weeks, 6 days))\n",
    "- Extract the most specific date information available\"\"\"\n",
    "    elif category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "        format_instructions = \"\"\"- For numeric values, provide precise values with appropriate decimal places\n",
    "- Round to 2-3 decimal places unless more precision is needed\n",
    "- Use standard decimal notation (e.g., 123.45)\n",
    "- For whole numbers, provide exact integer values\"\"\"\n",
    "    else:\n",
    "        format_instructions = \"\"\"- For numbers, use appropriate precision (integers for whole numbers, decimals for precise values)\n",
    "- For dates, use MM/DD/YYYY format\"\"\"\n",
    "    \n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a clinical calculation assistant. First provide the final answer, then your reasoning inside <thinking> tags.\n",
    "- Output format: \n",
    "Answer: <number or date only>\n",
    "<thinking>step-by-step reasoning here</thinking>\n",
    "- Do not include any text outside this format\n",
    "- Keep reasoning concise and focused on calculation steps\n",
    "- {format_instructions}\n",
    "- Ensure your answer falls within reasonable clinical ranges<|im_end|>\n",
    "<|im_start|>user\n",
    "{entities_section}Patient case:\n",
    "{patient_note}\n",
    "\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Sanity Outputs by Category\n",
    "\n",
    "- For a single example per category, prints raw generations for each model and method.\n",
    "- Useful to spot prompt formatting issues early before full-batch runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "_dLQeAwOLQ3c",
    "outputId": "648f054e-8979-4639-a8e8-b3799c063b62"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "categories = train_df['Category'].unique()\n",
    "\n",
    "with open(\"model_outputs.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for category in tqdm(categories, desc=\"Categories\"):\n",
    "        # Select a representative example for this category\n",
    "        example = train_df[train_df['Category'] == category].iloc[0]\n",
    "        \n",
    "        f.write(f\"\\n##### CATEGORY: {category} #####\\n\\n\")\n",
    "        for model_key in tqdm([\"1.7B\", \"0.6B\"], desc=f\"Models ({category})\", leave=False):\n",
    "            f.write(f\"===== MODEL {model_key} =====\\n\\n\")\n",
    "\n",
    "            # Zero-shot\n",
    "            zs_prompt = zero_shot_prompt(\n",
    "                example['Patient Note'], \n",
    "                example['Question'], \n",
    "                example[\"Relevant Entities\"]\n",
    "            )\n",
    "            zs_output = generate_answer(zs_prompt, models[model_key], tokenizers[model_key])\n",
    "            f.write(\"Zero-shot output:\\n\")\n",
    "            f.write(zs_output + \"\\n\")\n",
    "            f.write(\"------------------------------\\n\\n\")\n",
    "\n",
    "            # Few-shot\n",
    "            fs_prompt = few_shot_prompt(\n",
    "                example['Patient Note'], \n",
    "                example['Question'], \n",
    "                train_df, 3,\n",
    "                example[\"Relevant Entities\"]\n",
    "            )\n",
    "            fs_output = generate_answer(fs_prompt, models[model_key], tokenizers[model_key])\n",
    "            f.write(\"Few-shot output:\\n\")\n",
    "            f.write(fs_output + \"\\n\")\n",
    "            f.write(\"------------------------------\\n\\n\")\n",
    "\n",
    "            # CoT\n",
    "            cot_prompt = CoT(\n",
    "                example['Patient Note'], \n",
    "                example['Question'], \n",
    "                example[\"Relevant Entities\"]\n",
    "            )\n",
    "            cot_output = generate_answer(cot_prompt, models[model_key], tokenizers[model_key], max_length=256)\n",
    "            f.write(\"CoT output:\\n\")\n",
    "            f.write(cot_output + \"\\n\")\n",
    "            f.write(\"==============================\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batched Inference over Test Set\n",
    "\n",
    "- Runs all methods (zero-shot, few-shot, CoT) for both models.\n",
    "- Batching with left-padding to support decoder-only models.\n",
    "- Sampling: `temperature=0.3`, `top_p=0.9` to allow slight diversity but keep outputs stable.\n",
    "- Extracts `Answer:` from generations, parses by category, and validates within ground-truth range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492,
     "referenced_widgets": [
      "e6e7ca43d838432bb6d10b13d5a947fe",
      "dd08890a2bcc4bea9e1d8f295ad48c9d",
      "5a186ceda6324233860986498222e2ae",
      "acae790e32c74a82bd4c8b05f463e1f9",
      "2bbc6e995c1f4f63be50f5e9dad6e376",
      "2193aa8aa26f4f6098638667ff4c23e2",
      "fce5a57cb99049fa9fe98c4c9cf94c15",
      "28b118d7850d41d7a1ad8d473adbb3be",
      "b0f347bff23b40ae900077846b5441e7",
      "bebd4dc4632d48b784a1f2910a533114",
      "9186461e305f4e80ba48cbd1897d7688"
     ]
    },
    "id": "9mGMpAwxLQ3d",
    "outputId": "bba6f6af-4923-4df7-eb20-226ab1dcab17"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "OUTPUT_DIR = \"outputs\"  # absolute-like path; works in Colab/workspace root mounted with write perms\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Few-shot K balances context budget vs information signal; increase if VRAM allows\n",
    "N_FEW_SHOT = 3  # chosen to fit alongside long notes without truncation on T4\n",
    "# Batch size is VRAM-bound; 4 is typically safe on T4 for modest context windows\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def generate_batch(prompts, model, tokenizer, max_length=512):\n",
    "    \"\"\"Generate answers for a batch of prompts with controlled sampling.\n",
    "    - Left-padded inputs allow efficient batching.\n",
    "    - Low temperature keeps outputs stable; top_p avoids degenerate loops.\n",
    "    - max_length=512 accommodates CoT where needed while keeping GPU time bounded.\n",
    "    \"\"\"\n",
    "    # Tokenize a list of prompts with padding/truncation for batched generation\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    # Decoding parameters mirror single-prompt wrapper to maintain consistency\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_length,   # cap new tokens to control latency/VRAM\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "def run_inference(df, model_key, prompt_type, out_file, use_sampling=True, temperature=0.7):\n",
    "    \"\"\"Run dataset-wide inference for a given model and prompting method.\n",
    "    - Builds prompts per row using category-aware templates.\n",
    "    - Extracts `Answer:` then parses/validates into accuracy via `in_range`.\n",
    "    Decisions:\n",
    "    - Reload model per run to avoid cross-prompt-type interference and free VRAM in between.\n",
    "    - Left-padding and left-truncation align with earlier tokenizer setup for batching.\n",
    "    - `temperature` argument kept for future tuning hooks; internal default remains 0.3.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model {model_names[model_key]}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_names[model_key])\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_names[model_key]).to(\"cuda\").eval()\n",
    "\n",
    "    rows = []\n",
    "    batch_prompts = []\n",
    "    batch_indices = []\n",
    "\n",
    "    # Precompute all prompts first (reduces CPU/GPU idle time)\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{model_key}-{prompt_type}\"):\n",
    "        patient_note = row[\"Patient Note\"]\n",
    "        question = row[\"Question\"]\n",
    "        entities = row[\"Relevant Entities\"]\n",
    "        category = row.get(\"Category\", None)\n",
    "\n",
    "        if prompt_type == \"zero_shot\":\n",
    "            prompt = zero_shot_prompt(patient_note, question, entities, category)\n",
    "        elif prompt_type == \"few_shot\":\n",
    "            prompt = few_shot_prompt(patient_note, question, df, N_FEW_SHOT, entities, category)\n",
    "        elif prompt_type == \"cot\":\n",
    "            prompt = CoT(patient_note, question, entities, category)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prompt_type: {prompt_type}\")\n",
    "\n",
    "        batch_prompts.append(prompt)\n",
    "        batch_indices.append(idx)\n",
    "\n",
    "        # Generate in batches at fixed size to control VRAM\n",
    "        if len(batch_prompts) == BATCH_SIZE:\n",
    "            if prompt_type == \"cot\":\n",
    "                outputs = generate_batch(batch_prompts, model, tokenizer, max_length=256)\n",
    "            else:\n",
    "                outputs = generate_batch(batch_prompts, model, tokenizer)\n",
    "\n",
    "            for i, output in zip(batch_indices, outputs):\n",
    "                # Extract numeric/date answer and compute correctness\n",
    "                raw_answer = extract_answer_from_output(output)\n",
    "                parsed_answer = parse_answer(raw_answer, category)\n",
    "                in_range = validate_answer_range(\n",
    "                    parsed_answer,\n",
    "                    df.loc[i, \"Lower Limit\"],\n",
    "                    df.loc[i, \"Upper Limit\"],\n",
    "                    category,\n",
    "                )\n",
    "\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"index\": i,\n",
    "                        \"id\": df.loc[i, \"Note ID\"],\n",
    "                        \"model\": model_key,\n",
    "                        \"prompt_type\": prompt_type,\n",
    "                        \"category\": df.loc[i, \"Category\"],\n",
    "                        \"patient_note\": df.loc[i, \"Patient Note\"],\n",
    "                        \"question\": df.loc[i, \"Question\"],\n",
    "                        \"entities\": df.loc[i, \"Relevant Entities\"],\n",
    "                        \"ground_truth\": df.loc[i, \"Ground Truth Answer\"],\n",
    "                        \"lower_limit\": df.loc[i, \"Lower Limit\"],\n",
    "                        \"upper_limit\": df.loc[i, \"Upper Limit\"],\n",
    "                        \"raw_output\": output,\n",
    "                        \"extracted_answer\": raw_answer,\n",
    "                        \"parsed_answer\": parsed_answer,\n",
    "                        \"in_range\": in_range,\n",
    "                    }\n",
    "                )\n",
    "            batch_prompts, batch_indices = [], []\n",
    "\n",
    "    # Process remaining prompts (tail batch)\n",
    "    if batch_prompts:\n",
    "        if prompt_type == \"cot\":\n",
    "            outputs = generate_batch(batch_prompts, model, tokenizer, max_length=256)\n",
    "        else:\n",
    "            outputs = generate_batch(batch_prompts, model, tokenizer)\n",
    "\n",
    "        for i, output in zip(batch_indices, outputs):\n",
    "            raw_answer = extract_answer_from_output(output)\n",
    "            parsed_answer = parse_answer(raw_answer, category)\n",
    "            in_range = validate_answer_range(\n",
    "                parsed_answer,\n",
    "                df.loc[i, \"Lower Limit\"],\n",
    "                df.loc[i, \"Upper Limit\"],\n",
    "                category,\n",
    "            )\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"index\": i,\n",
    "                    \"id\": df.loc[i, \"Note ID\"],\n",
    "                    \"model\": model_key,\n",
    "                    \"prompt_type\": prompt_type,\n",
    "                    \"category\": df.loc[i, \"Category\"],\n",
    "                    \"patient_note\": df.loc[i, \"Patient Note\"],\n",
    "                    \"question\": df.loc[i, \"Question\"],\n",
    "                    \"entities\": df.loc[i, \"Relevant Entities\"],\n",
    "                    \"ground_truth\": df.loc[i, \"Ground Truth Answer\"],\n",
    "                    \"lower_limit\": df.loc[i, \"Lower Limit\"],\n",
    "                    \"upper_limit\": df.loc[i, \"Upper Limit\"],\n",
    "                    \"raw_output\": output,\n",
    "                    \"extracted_answer\": raw_answer,\n",
    "                    \"parsed_answer\": parsed_answer,\n",
    "                    \"in_range\": in_range,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Save results to CSV for downstream aggregation\n",
    "    pd.DataFrame(rows).to_csv(out_file, index=False)\n",
    "\n",
    "    # Unload model and clear cache to avoid OOM in subsequent runs\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Run all combinations with improved parameters for both models\n",
    "for prompt_type in [\"zero_shot\", \"cot\", \"few_shot\"]:\n",
    "    for model_key in [\"0.6B\", \"1.7B\"]:\n",
    "        out_file = f\"{OUTPUT_DIR}/{model_key}_{prompt_type}_improved.csv\"\n",
    "        if os.path.exists(out_file):\n",
    "            print(f\"Skipping → {out_file} (already exists)\")\n",
    "            continue\n",
    "        print(f\"Saving → {out_file}\")\n",
    "        run_inference(\n",
    "            test_df,\n",
    "            model_key,\n",
    "            prompt_type,\n",
    "            out_file,\n",
    "            use_sampling=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Design Rationale and Comparison\n",
    "\n",
    "**Prompt Engineering Strategy:**\n",
    "We implement three distinct prompting approaches to evaluate their effectiveness on clinical calculation tasks:\n",
    "\n",
    "#### 1. Zero-Shot Prompting\n",
    "**Design Rationale:**\n",
    "- **Strict formatting**: Enforces `Answer: <value>` format to ensure consistent parsing\n",
    "- **Category-specific instructions**: Tailored output format guidance based on data type (numeric vs. date vs. gestational age)\n",
    "- **Clinical context awareness**: Includes relevant entities and emphasizes clinical reasonableness\n",
    "- **Why this works**: Direct instruction-following leverages the model's pre-trained capabilities without additional examples\n",
    "\n",
    "#### 2. Few-Shot Prompting  \n",
    "**Design Rationale:**\n",
    "- **K=3 examples**: Balanced between context budget and information signal (fits T4 VRAM constraints)\n",
    "- **In-context learning**: Demonstrates the exact format and reasoning pattern expected\n",
    "- **Category-aware examples**: Shows diverse cases from the same category to improve generalization\n",
    "- **Why this works**: Provides concrete examples of successful task completion, reducing ambiguity\n",
    "\n",
    "#### 3. Chain-of-Thought (CoT) Prompting\n",
    "**Design Rationale:**\n",
    "- **Structured reasoning**: Captures step-by-step clinical calculation process\n",
    "- **`<thinking>` tags**: Isolates reasoning from final answer for clean evaluation\n",
    "- **Clinical methodology**: Encourages systematic approach to medical calculations\n",
    "- **Why this works**: Forces the model to explicitly work through the calculation steps, improving accuracy\n",
    "\n",
    "#### Prompt Effectiveness Comparison:\n",
    "- **Zero-shot**: Fastest, baseline performance, good for simple cases\n",
    "- **Few-shot**: Better accuracy through examples, moderate computational cost\n",
    "- **CoT**: Highest accuracy for complex reasoning, highest computational cost\n",
    "- **RAG**: Combines retrieval with few-shot benefits, best for diverse cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGIOTAixRK8l",
    "outputId": "861b561d-2eaf-4627-f049-133d7c74c67b"
   },
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter-Efficient Fine-Tuning (PEFT) Configuration\n",
    "\n",
    "**LoRA/QLoRA Parameter Choices and Rationale:**\n",
    "\n",
    "#### Quantization Strategy (QLoRA)\n",
    "- **4-bit NF4 quantization**: Reduces memory footprint by ~75% while maintaining model quality\n",
    "- **Double quantization**: Further reduces memory overhead for adapter weights\n",
    "- **Float16 compute dtype**: Optimal balance between speed and memory on T4 GPUs\n",
    "- **Why QLoRA**: Enables fine-tuning large models on consumer hardware while preserving performance\n",
    "\n",
    "#### LoRA Configuration\n",
    "- **Rank (r=16)**: Sufficient capacity for task-specific adaptation without overfitting\n",
    "- **Alpha (α=16)**: Matches rank for stable training (α/r = 1.0 scaling factor)\n",
    "- **Target modules**: `[\"c_attn\", \"o_proj\", \"w1\", \"w2\"]` - covers attention and MLP layers\n",
    "- **Dropout (0.05)**: Prevents overfitting on small datasets\n",
    "- **Why these parameters**: Proven effective for instruction-following tasks while maintaining efficiency\n",
    "\n",
    "#### Training Configuration\n",
    "- **Batch size**: 2 with gradient accumulation (effective batch size = 4)\n",
    "- **Learning rate**: 2e-5 (conservative for stable fine-tuning)\n",
    "- **Epochs**: 3 (sufficient for convergence without overfitting)\n",
    "- **FP16**: Memory efficiency while maintaining training stability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJG3372zLQ3g"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Define your 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Formatting and Tokenization\n",
    "\n",
    "- Builds zero-shot training pairs with strict output format `Answer: <value>`.\n",
    "- For dates: normalizes to MM/DD/YYYY or (weeks, days) when present.\n",
    "- Masks prompt tokens in `labels` as -100 so loss focuses on target answer only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAbNd0UMLQ3h"
   },
   "outputs": [],
   "source": [
    "def get_peft_config(model_name):\n",
    "    return LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"c_attn\", \"o_proj\", \"w1\", \"w2\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "def build_zero_shot(example, tokenizer=None):\n",
    "    patient_note = example[\"Patient Note\"]\n",
    "    question = example[\"Question\"]\n",
    "    answer = example[\"Ground Truth Answer\"]\n",
    "    entities = example.get(\"Relevant Entities\", \"\")\n",
    "    category = example.get(\"Category\", None)\n",
    "\n",
    "    entities_section = f\"Relevant Entities: {entities}\\n\\n\" if entities else \"\"\n",
    "\n",
    "    if category == \"date\":\n",
    "        format_instructions = \"- For dates, use MM/DD/YYYY format\\n- If only year is available, use 01/01/YYYY\\n- For gestational age, use (X weeks, Y days)\"\n",
    "    elif category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "        format_instructions = \"- Provide precise numeric values\\n- Round to 2-3 decimal places unless more precision is needed\\n- Use standard decimal notation\"\n",
    "    else:\n",
    "        format_instructions = \"- For numbers, use appropriate precision\\n- For dates, use MM/DD/YYYY\\n- For gestational age, use (X weeks, Y days)\"\n",
    "\n",
    "    input_text = f\"\"\"<|im_start|>system\n",
    "You are a clinical calculation assistant. Extract only the numerical answer or date.\n",
    "- Output format: Answer: <number or date only>\n",
    "- Do not explain or add any text\n",
    "- {format_instructions}\n",
    "- Ensure your answer is clinically reasonable<|im_end|>\n",
    "<|im_start|>user\n",
    "{entities_section}Patient case:\n",
    "{patient_note}\n",
    "\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer: \"\"\"\n",
    "\n",
    "    # Format answer\n",
    "    if pd.isna(answer) or str(answer).strip() == \"\":\n",
    "        formatted_answer = \"unknown\"\n",
    "    else:\n",
    "        if category == \"date\":\n",
    "            try:\n",
    "                match = re.search(r'\\((\\d+)\\s*weeks?[, ]*(\\d+)\\s*days?\\)', str(answer), re.IGNORECASE)\n",
    "                if match:\n",
    "                    weeks, days = map(int, match.groups())\n",
    "                    formatted_answer = f\"({weeks} weeks, {days} days)\"\n",
    "                else:\n",
    "                    date_formats = [\"%m/%d/%Y\", \"%m/%d/%y\", \"%Y-%m-%d\", \"%d/%m/%Y\", \"%Y\"]\n",
    "                    parsed_date = None\n",
    "                    for fmt in date_formats:\n",
    "                        try:\n",
    "                            parsed_date = datetime.strptime(str(answer), fmt)\n",
    "                            break\n",
    "                        except:\n",
    "                            continue\n",
    "                    if parsed_date:\n",
    "                        formatted_answer = parsed_date.strftime(\"%m/%d/%Y\")\n",
    "                    else:\n",
    "                        year_match = re.search(r'(\\d{4})', str(answer))\n",
    "                        formatted_answer = f\"01/01/{year_match.group(1)}\" if year_match else str(answer).strip()\n",
    "            except:\n",
    "                formatted_answer = str(answer).strip()\n",
    "        else:\n",
    "            formatted_answer = str(answer).strip()\n",
    "\n",
    "    return {\n",
    "        \"input_text\": input_text.strip(),\n",
    "        \"labels\": f\"{formatted_answer}\".strip()\n",
    "    }\n",
    "\n",
    "def tokenize_function(example, tokenizer):\n",
    "    zero_shot = build_zero_shot(example, tokenizer)\n",
    "    \n",
    "    # Tokenize prompt and answer separately but consistently\n",
    "    prompt_tokens = tokenizer(\n",
    "        zero_shot[\"input_text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        add_special_tokens=True,\n",
    "        return_offsets_mapping=False\n",
    "    )\n",
    "    \n",
    "    answer_tokens = tokenizer(\n",
    "        zero_shot[\"labels\"], \n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=False\n",
    "    )\n",
    "    \n",
    "    # Combine them manually to ensure alignment\n",
    "    input_ids = prompt_tokens[\"input_ids\"] + answer_tokens[\"input_ids\"]\n",
    "    \n",
    "    # Create labels: -100 for prompt, actual tokens for answer\n",
    "    labels = [-100] * len(prompt_tokens[\"input_ids\"]) + answer_tokens[\"input_ids\"]\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Implementation (PEFT + QLoRA)\n",
    "\n",
    "**Training Process:**\n",
    "- Applies LoRA on attention/MLP projections with r=16, alpha=16, dropout=0.05\n",
    "- Uses 4-bit NF4 quantization for memory efficiency (QLoRA)\n",
    "- Tokenization masks the prompt portion (labels=-100) to train only on the target string\n",
    "- TrainingArgs tuned for stability under limited VRAM (batch 4, grad_accum 8, 3 epochs)\n",
    "\n",
    "**How Parameter Choices Affect Results:**\n",
    "\n",
    "#### LoRA Rank Impact\n",
    "- **r=16**: Provides sufficient capacity for task adaptation without overfitting\n",
    "- **Lower rank (r=8)**: Faster training but potentially limited expressiveness\n",
    "- **Higher rank (r=32)**: More parameters but risk of overfitting on small datasets\n",
    "\n",
    "#### Quantization Effects\n",
    "- **4-bit vs 8-bit**: 4-bit saves more memory but may slightly reduce precision\n",
    "- **NF4 vs standard 4-bit**: NF4 maintains better quality with same memory footprint\n",
    "- **Double quantization**: Additional memory savings with minimal quality loss\n",
    "\n",
    "#### Training Configuration Impact\n",
    "- **Learning rate (2e-5)**: Conservative choice prevents catastrophic forgetting\n",
    "- **Batch size (2)**: Fits T4 memory while maintaining gradient stability\n",
    "- **Epochs (3)**: Sufficient for convergence without overfitting on clinical data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531,
     "referenced_widgets": [
      "e4862d4872a1492088c2a3a0d943ae74",
      "9d20d912668245d483fb35e33a2d9555",
      "6ef7346f78a444a282edf65a4c2479e6",
      "4daff3b9a2af4cdcb26300d230ecfc94",
      "801b36af9e7045e596f8b719cef3e3f0",
      "ccac27e3c2bf45e7af1e81fe577e32ed",
      "7217115a6f304797919d81a23968b486",
      "ae1bc63b4e8c42b896f6c9999bcde18b",
      "2def2041cb114c1db533ead31a234748",
      "cd985c75884a4ba5992726c376a0f0b8",
      "23524eeb480b48dca2d7be051d60ef5a"
     ]
    },
    "id": "lgFYtEpfLQ3i",
    "outputId": "abeb8349-f748-4b98-eb60-644dcac9bb90"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "train_hf = dataset[\"train\"]\n",
    "\n",
    "# Safe collator that handles missing attention_mask or labels\n",
    "def get_collate_fn(tokenizer):\n",
    "    def collate_fn(batch):\n",
    "        input_ids = [torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch]\n",
    "        attention_mask = [torch.ones_like(ids, dtype=torch.long) for ids in input_ids]\n",
    "        \n",
    "        labels = []\n",
    "        for x in batch:\n",
    "            if \"labels\" in x:\n",
    "                labels.append(torch.tensor(x[\"labels\"], dtype=torch.long))\n",
    "            else:\n",
    "                labels.append(torch.full_like(torch.tensor(x[\"input_ids\"], dtype=torch.long), -100))\n",
    "\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "        labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "    return collate_fn\n",
    "\n",
    "for key, name in model_names.items():\n",
    "    print(f\"\\n=== Training {name} with LoRA/QLoRA ===\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    collate_fn = get_collate_fn(tokenizer)  # Create collator with tokenizer\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    # Apply LoRA\n",
    "    peft_config = get_peft_config(name)\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_dataset = train_hf.map(\n",
    "        lambda x: tokenize_function(x, tokenizer),\n",
    "        batched=False\n",
    "    )\n",
    "\n",
    "    print(\"\\nPROMPT EXAMPLE CHECK:\")\n",
    "    sample_example = train_hf[0]  # Get first example\n",
    "    zero_shot_example = build_zero_shot(sample_example)\n",
    "    \n",
    "    print(\"=== INPUT TEXT ===\")\n",
    "    print(zero_shot_example[\"input_text\"])\n",
    "    print(\"\\n=== EXPECTED ANSWER ===\")\n",
    "    print(zero_shot_example[\"labels\"])\n",
    "    print(\"\\n=== TOKENIZED LENGTHS ===\")\n",
    "    tokenized_example = tokenize_function(sample_example, tokenizer)\n",
    "    print(f\"Input IDs length: {len(tokenized_example['input_ids'])}\")\n",
    "    print(f\"Labels length: {len(tokenized_example['labels'])}\")\n",
    "    print(f\"Non-masked labels: {sum(1 for x in tokenized_example['labels'] if x != -100)}\")\n",
    "    \n",
    "    # Optional: Print first few tokens to verify masking\n",
    "    print(\"\\n=== FIRST 20 TOKENS (for sanity check) ===\")\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(tokenized_example[\"input_ids\"][:20])\n",
    "    label_tokens = tokenized_example[\"labels\"][:20]\n",
    "    for i, (input_tok, label_val) in enumerate(zip(input_tokens, label_tokens)):\n",
    "        mask_status = \"MASKED\" if label_val == -100 else \"TRAINED\"\n",
    "        print(f\"Token {i}: {input_tok} -> {mask_status}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./qwen_lora_{key}\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        save_steps=100,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=collate_fn\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    print(f\"✅ Finished training {name}\")\n",
    "\n",
    "    # Save LoRA adapter\n",
    "    lora_save_path = Path(f\"./qwen_lora_{key}/adapter\")\n",
    "    lora_save_path.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(lora_save_path)\n",
    "    print(f\"LoRA adapter saved to {lora_save_path}\")\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(f\"./qwen_lora_{key}/tokenizer\")\n",
    "    print(f\"Tokenizer saved to ./qwen_lora_{key}/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized Inference with LoRA Adapters (QLoRA)\n",
    "\n",
    "- Loads 4-bit quantized base model and merges trained LoRA adapters for inference.\n",
    "- Rationale: fit within Colab T4 memory while retaining fine-tuned behavior.\n",
    "- Method: Zero-shot prompting on merged model; outputs saved per model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e585f8a7"
   },
   "outputs": [],
   "source": [
    "# Run inference on quantized models with saved LoRA adapters\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Define your 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "N_FEW_SHOT = 5\n",
    "BATCH_SIZE = 4  # Adjust depending on your GPU memory\n",
    "\n",
    "def generate_batch(prompts, model, tokenizer, max_new_tokens=32):\n",
    "    # determine model context size\n",
    "    max_context = getattr(model.config, \"max_position_embeddings\", None) or getattr(model.config, \"max_length\", 2048)\n",
    "    # ensure there's room for the requested new tokens\n",
    "    max_input_len = max(1, max_context - max_new_tokens)\n",
    "\n",
    "    # tokenize with left truncation so we keep the most recent part of the prompt\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tok = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_len)\n",
    "\n",
    "    # move inputs to the model device (works when device_map=\"auto\")\n",
    "    device = next(model.parameters()).device\n",
    "    tok = {k: v.to(device) for k, v in tok.items()}\n",
    "\n",
    "    # set sensible EOS / PAD ids\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else eos_id\n",
    "\n",
    "    out = model.generate(\n",
    "        **tok,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=eos_id,\n",
    "        pad_token_id=pad_id\n",
    "        #truncation_side=\"left\" \n",
    "    )\n",
    "\n",
    "    # compute per-example input lengths (non-pad tokens)\n",
    "    input_lengths = (tok[\"input_ids\"] != pad_id).sum(dim=1).tolist()\n",
    "\n",
    "    # extract only the newly generated tokens for each example\n",
    "    results = []\n",
    "    for i in range(out.size(0)):\n",
    "        start = input_lengths[i]\n",
    "        gen_tokens = out[i, start:] if start < out.size(1) else out[i, :]\n",
    "        results.append(tokenizer.decode(gen_tokens, skip_special_tokens=True).strip())\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def run_quantized_inference_from_lora(df, model_key, out_file):\n",
    "    # Load base model with quantization\n",
    "    print(f\"Loading base quantized model {model_names[model_key]}...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_names[model_key],\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    # Load tokenizer from saved LoRA path\n",
    "    tokenizer_path = f\"./qwen_lora_{model_key}/tokenizer\"\n",
    "    print(f\"Loading tokenizer from {tokenizer_path}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "\n",
    "    # Ensure pad_token exists\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load LoRA adapter\n",
    "    lora_path = f\"./qwen_lora_{model_key}/adapter\"\n",
    "    print(f\"Loading LoRA adapter from {lora_path}...\")\n",
    "    model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    model.eval()\n",
    "\n",
    "    rows = []\n",
    "    batch_prompts = []\n",
    "    batch_indices = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{model_key}-quantized\"):\n",
    "        zero_shot_example = build_zero_shot(row)\n",
    "        prompt = zero_shot_example[\"input_text\"]\n",
    "\n",
    "        batch_prompts.append(prompt)\n",
    "        batch_indices.append(idx)\n",
    "\n",
    "        if len(batch_prompts) == BATCH_SIZE:\n",
    "            outputs = generate_batch(batch_prompts, model, tokenizer, max_new_tokens=32)\n",
    "            for i, output in zip(batch_indices, outputs):\n",
    "                if \"Answer:\" not in output:\n",
    "                    output = f\"Answer: {output.strip()}\"\n",
    "                raw_answer = extract_answer_from_output(output)\n",
    "                parsed_answer = parse_answer(raw_answer, df.loc[i, \"Category\"])\n",
    "                in_range = validate_answer_range(parsed_answer, \n",
    "                                                 df.loc[i, \"Lower Limit\"], \n",
    "                                                 df.loc[i, \"Upper Limit\"], \n",
    "                                                 df.loc[i, \"Category\"])\n",
    "                rows.append({\n",
    "                    \"index\": i,\n",
    "                    \"id\": df.loc[i, \"Note ID\"],\n",
    "                    \"model\": model_key,\n",
    "                    \"prompt_type\": \"zero_shot_quantized_lora\",\n",
    "                    \"category\": df.loc[i, \"Category\"],\n",
    "                    \"patient_note\": df.loc[i, \"Patient Note\"],\n",
    "                    \"question\": df.loc[i, \"Question\"],\n",
    "                    \"entities\": df.loc[i, \"Relevant Entities\"],\n",
    "                    \"ground_truth\": df.loc[i, \"Ground Truth Answer\"],\n",
    "                    \"lower_limit\": df.loc[i, \"Lower Limit\"],\n",
    "                    \"upper_limit\": df.loc[i, \"Upper Limit\"],\n",
    "                    \"raw_output\": output,\n",
    "                    \"extracted_answer\": raw_answer,\n",
    "                    \"parsed_answer\": parsed_answer,\n",
    "                    \"in_range\": in_range\n",
    "                })\n",
    "            batch_prompts, batch_indices = [], []\n",
    "\n",
    "    # Process remaining prompts\n",
    "    if batch_prompts:\n",
    "        outputs = generate_batch(batch_prompts, model, tokenizer, max_new_tokens=32)\n",
    "        for i, output in zip(batch_indices, outputs):\n",
    "            raw_answer = extract_answer_from_output(output)\n",
    "            parsed_answer = parse_answer(raw_answer, df.loc[i, \"Category\"])\n",
    "            in_range = validate_answer_range(parsed_answer, \n",
    "                                             df.loc[i, \"Lower Limit\"], \n",
    "                                             df.loc[i, \"Upper Limit\"], \n",
    "                                             df.loc[i, \"Category\"])\n",
    "            rows.append({\n",
    "                \"index\": i,\n",
    "                \"id\": df.loc[i, \"Note ID\"],\n",
    "                \"model\": model_key,\n",
    "                \"prompt_type\": \"zero_shot_quantized_lora\",\n",
    "                \"category\": df.loc[i, \"Category\"],\n",
    "                \"patient_note\": df.loc[i, \"Patient Note\"],\n",
    "                \"question\": df.loc[i, \"Question\"],\n",
    "                \"entities\": df.loc[i, \"Relevant Entities\"],\n",
    "                \"ground_truth\": df.loc[i, \"Ground Truth Answer\"],\n",
    "                \"lower_limit\": df.loc[i, \"Lower Limit\"],\n",
    "                \"upper_limit\": df.loc[i, \"Upper Limit\"],\n",
    "                \"raw_output\": output,\n",
    "                \"extracted_answer\": raw_answer,\n",
    "                \"parsed_answer\": parsed_answer,\n",
    "                \"in_range\": in_range\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_file, index=False)\n",
    "\n",
    "    # Unload model and clear cache\n",
    "    del model\n",
    "    del base_model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Run inference\n",
    "if 'test_df' in locals():\n",
    "    for model_key in [\"0.6B\", \"1.7B\"]:\n",
    "        out_file = f\"{OUTPUT_DIR}/{model_key}_zero_shot_quantized_lora.csv\"\n",
    "        if os.path.exists(out_file):\n",
    "            print(f\"Skipping → {out_file} (already exists)\")\n",
    "            continue\n",
    "        print(f\"Saving → {out_file}\")\n",
    "        run_quantized_inference_from_lora(test_df, model_key, out_file)\n",
    "else:\n",
    "    print(\"Error: test_df not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**RAG Implementation Strategy:**\n",
    "\n",
    "#### Retrieval System\n",
    "- **Embedding model**: `all-MiniLM-L6-v2` - lightweight, fast, effective for clinical text\n",
    "- **Retrieval count**: `N_RETRIEVAL=5` - balanced between context and diversity\n",
    "- **Similarity metric**: Cosine similarity on combined patient note + question embeddings\n",
    "- **Why this works**: Retrieves clinically similar cases to provide relevant examples\n",
    "\n",
    "#### RAG Prompt Construction\n",
    "- **Context combination**: Patient note + question + relevant entities for retrieval\n",
    "- **Example integration**: Includes retrieved (note, question, answer) pairs as few-shot examples\n",
    "- **Format consistency**: Maintains same output format as other prompting methods\n",
    "- **Why this works**: Provides task-specific examples without manual curation\n",
    "\n",
    "#### Generation Parameters\n",
    "- **Temperature (0.3)**: Low randomness for consistent clinical outputs\n",
    "- **Top-p (0.9)**: Nucleus sampling for controlled diversity\n",
    "- **Max tokens (32)**: Concise answers to avoid hallucination\n",
    "- **Why these settings**: Balance between creativity and clinical accuracy\n",
    "\n",
    "**RAG vs Other Methods:**\n",
    "- **vs Zero-shot**: Better performance through relevant examples\n",
    "- **vs Few-shot**: Dynamic example selection based on similarity\n",
    "- **vs CoT**: Faster than explicit reasoning, similar accuracy\n",
    "- **vs Fine-tuning**: No training required, adapts to new cases automatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "N_RETRIEVAL = 5   # number of neighbors retrieved\n",
    "\n",
    "# --- Load Embedding Model ---\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# --- Load Data ---\n",
    "dataset = load_dataset(\"ncbi/MedCalc-Bench-v1.0\")\n",
    "\n",
    "# Split into train/test\n",
    "train_df = dataset[\"train\"].to_pandas()\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "# --- Precompute Train Embeddings (if not already saved) ---\n",
    "if not os.path.exists(\"rag_train_embeddings.npy\"):\n",
    "    train_texts = (\n",
    "        train_df[\"Patient Note\"] + \" \" +\n",
    "        train_df[\"Question\"] + \" \" +\n",
    "        (train_df[\"Relevant Entities\"].fillna(\"\") if \"Relevant Entities\" in train_df else \"\") + \" \" +\n",
    "        (train_df[\"Ground Truth Explanation\"].fillna(\"\") if \"Ground Truth Explanation\" in train_df else \"\")\n",
    "    )\n",
    "    train_embeddings = embedding_model.encode(train_texts.tolist(), convert_to_numpy=True)\n",
    "    np.save(\"rag_train_embeddings.npy\", train_embeddings)\n",
    "\n",
    "def load_rag_embeddings(embed_file=\"rag_train_embeddings.npy\"):\n",
    "    return np.load(embed_file)\n",
    "\n",
    "# --- Retrieval ---\n",
    "def retrieve_examples(note, question, k=N_RETRIEVAL):\n",
    "    rag_embeds = load_rag_embeddings()\n",
    "    text = note + \" \" + question\n",
    "    test_embed = embedding_model.encode([text], convert_to_numpy=True)\n",
    "    sims = np.dot(rag_embeds, test_embed[0])\n",
    "    idxs = sims.argsort()[-k:][::-1]\n",
    "    return train_df.iloc[idxs]\n",
    "\n",
    "# --- Prompt Builder ---\n",
    "def rag_prompt(patient_note, question, examples, n=None, entities_json=None, category=None, include_explanations=False):\n",
    "    \"\"\"\n",
    "    Build a RAG-style prompt with improved formatting.\n",
    "    - examples: a DataFrame (retrieved examples); will include up to n examples (default: all)\n",
    "    - entities_json: optional string for Relevant Entities\n",
    "    - category: category of the question (lab, risk, physical, severity, diagnosis, date, dosage)\n",
    "    \"\"\"\n",
    "    if n is None:\n",
    "        n = len(examples)\n",
    "\n",
    "    entities_section = f\"Relevant Entities: {entities_json}\\n\\n\" if entities_json else \"\"\n",
    "\n",
    "    # Determine format instructions based on category\n",
    "    if category == \"date\":\n",
    "        format_instructions = \"\"\"- For dates, use MM/DD/YYYY format (e.g., 08/31/2023)\n",
    "- If only year is available, use 01/01/YYYY\n",
    "- If year and month are available, use MM/01/YYYY\n",
    "- For gestational age or time periods, use (X weeks, Y days) format (e.g., (0 weeks, 6 days))\n",
    "- Extract the most specific date information available\"\"\"\n",
    "    elif category in [\"lab\", \"risk\", \"physical\", \"severity\", \"diagnosis\", \"dosage\"]:\n",
    "        format_instructions = \"\"\"- For numeric values, provide precise values with appropriate decimal places\n",
    "- Round to 2-3 decimal places unless more precision is needed\n",
    "- Use standard decimal notation (e.g., 123.45)\n",
    "- For whole numbers, provide exact integer values\"\"\"\n",
    "    else:\n",
    "        format_instructions = \"\"\"- For numbers, use appropriate precision (integers for whole numbers, decimals for precise values)\n",
    "- For dates, use MM/DD/YYYY format\n",
    "- For gestational age or time periods, use (X weeks, Y days) format (e.g., (0 weeks, 6 days))\"\"\"\n",
    "\n",
    "    system_block = f\"\"\"<|im_start|>system\n",
    "You are a clinical calculation assistant. Extract only the numerical answer or date from the patient case.\n",
    "- Output format: Answer: <number or date only>\n",
    "- Do not explain, justify, or add any text\n",
    "- Do not repeat the question\n",
    "- Use the retrieved examples as guidance only\n",
    "- Follow the exact format of the examples below\n",
    "- {format_instructions}\n",
    "- Ensure your answer falls within reasonable clinical ranges\n",
    "- If unsure, provide your best numerical estimate<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = system_block\n",
    "\n",
    "    # Add retrieved examples as few-shot user/assistant pairs\n",
    "    for i in range(min(n, len(examples))):\n",
    "        ex = examples.iloc[i].to_dict()\n",
    "\n",
    "        ex_note = ex.get(\"Patient Note\", \"\") or \"\"\n",
    "        ex_question = ex.get(\"Question\", \"\") or \"\"\n",
    "        ex_answer = ex.get(\"Ground Truth Answer\", \"\")\n",
    "        if pd.isna(ex_answer) or str(ex_answer).strip() == \"\":\n",
    "            ex_answer_str = \"unknown\"\n",
    "        else:\n",
    "            ex_answer_str = str(ex_answer).strip()\n",
    "\n",
    "        prompt += f\"\"\"<|im_start|>user\n",
    "Patient case:\n",
    "{ex_note}\n",
    "\n",
    "Question: {ex_question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer: {ex_answer_str}<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "    # Target example (the one to answer)\n",
    "    prompt += f\"\"\"<|im_start|>user\n",
    "{entities_section}Patient case:\n",
    "{patient_note}\n",
    "\n",
    "Question: {question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Answer:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# --- RAG Inference over Dataset ---\n",
    "def run_rag_inference(df, model_key, out_file, batch_size=2, use_sampling=True, temperature=0.7):\n",
    "    print(f\"Loading model {model_names[model_key]}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_names[model_key],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    ).eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_names[model_key])\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "\n",
    "    rows = []\n",
    "    batch_prompts = []\n",
    "    batch_meta = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"{model_key}-RAG\"):\n",
    "        patient_note = row[\"Patient Note\"]\n",
    "        question = row[\"Question\"]\n",
    "        entities = row.get(\"Relevant Entities\", \"\")\n",
    "        category = row.get(\"Category\", None)\n",
    "        retrieved = retrieve_examples(patient_note, question, k=N_RETRIEVAL)\n",
    "\n",
    "        prompt = rag_prompt(\n",
    "            patient_note,\n",
    "            question,\n",
    "            retrieved,\n",
    "            entities_json=entities,\n",
    "            category=category\n",
    "        )\n",
    "\n",
    "        batch_prompts.append(prompt)\n",
    "        batch_meta.append((idx, row, retrieved))\n",
    "\n",
    "        # Process when batch full\n",
    "        if len(batch_prompts) == batch_size:\n",
    "            rows.extend(_process_batch(batch_prompts, batch_meta, model, tokenizer, model_key, \n",
    "                                     use_sampling=use_sampling, temperature=temperature))\n",
    "            batch_prompts, batch_meta = [], []\n",
    "\n",
    "    # Process remainder\n",
    "    if batch_prompts:\n",
    "        rows.extend(_process_batch(batch_prompts, batch_meta, model, tokenizer, model_key,\n",
    "                                 use_sampling=use_sampling, temperature=temperature))\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_file, index=False)\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def _process_batch(batch_prompts, batch_meta, model, tokenizer, model_key, use_sampling=True, temperature=0.7):\n",
    "    \"\"\"Helper to run a batch of prompts with improved sampling.\"\"\"\n",
    "    results = []\n",
    "    inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,   # very low randomness\n",
    "        top_p=0.9,         # nucleus sampling but constrained\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    for (idx, row, retrieved), sample in zip(batch_meta, decoded):\n",
    "        raw_answer = extract_answer_from_output(sample)\n",
    "        category = row.get(\"Category\", None)\n",
    "        parsed_answer = parse_answer(raw_answer, category)\n",
    "        in_range = validate_answer_range(parsed_answer, \n",
    "                                       row.get(\"Lower Limit\"), \n",
    "                                       row.get(\"Upper Limit\"), \n",
    "                                       category)\n",
    "\n",
    "        results.append({\n",
    "            \"index\": idx,\n",
    "            \"id\": row.get(\"Note ID\"),\n",
    "            \"model\": model_key,\n",
    "            \"prompt_type\": \"rag\",\n",
    "            \"category\": row.get(\"Category\"),\n",
    "            \"patient_note\": row[\"Patient Note\"],\n",
    "            \"question\": row[\"Question\"],\n",
    "            \"entities\": row.get(\"Relevant Entities\", \"\"),\n",
    "            \"ground_truth\": row.get(\"Ground Truth Answer\"),\n",
    "            \"lower_limit\": row.get(\"Lower Limit\"),\n",
    "            \"upper_limit\": row.get(\"Upper Limit\"),\n",
    "            \"retrieved_ids\": list(retrieved[\"Note ID\"]) if \"Note ID\" in retrieved else None,\n",
    "            \"raw_output\": sample,\n",
    "            \"extracted_answer\": raw_answer,\n",
    "            \"parsed_answer\": parsed_answer,\n",
    "            \"in_range\": in_range\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- Run for both models with improved parameters ---\n",
    "for model_key in [\"0.6B\", \"1.7B\"]:\n",
    "    out_file = f\"{OUTPUT_DIR}/{model_key}_rag_improved.csv\"\n",
    "    if os.path.exists(out_file):\n",
    "        print(f\"Skipping → {out_file}\")\n",
    "        continue\n",
    "    print(f\"Saving → {out_file}\")\n",
    "    # Use sampling with moderate temperature for better variability\n",
    "    run_rag_inference(test_df, model_key, out_file, \n",
    "                     use_sampling=True, temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Methodology and Metrics\n",
    "\n",
    "**Evaluation Strategy:**\n",
    "- Loads all output CSVs from `outputs` directory\n",
    "- Uses `in_range` as the correctness indicator (1 if parsed answer within ground-truth range, else 0)\n",
    "- Aggregates accuracy per category for each Model × Method combination\n",
    "- Produces comprehensive results tables and sample counts for transparency\n",
    "\n",
    "**Accuracy Calculation:**\n",
    "- **Range-based validation**: Answers must fall within provided lower/upper limits\n",
    "- **Category-aware parsing**: Different validation rules for numeric vs. date vs. gestational age\n",
    "- **Conservative scoring**: Missing or invalid answers marked as incorrect\n",
    "- **Per-category breakdown**: Enables identification of model strengths/weaknesses\n",
    "\n",
    "**Results Aggregation:**\n",
    "- **Per-model accuracy**: Overall performance across all categories\n",
    "- **Per-category accuracy**: Performance breakdown by clinical calculation type\n",
    "- **Sample counts**: Ensures statistical significance of results\n",
    "- **Comparative analysis**: Direct comparison of prompting methods and model sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to find all CSV files in outputs directory\n",
    "def find_csv_files():\n",
    "    \"\"\"Find all CSV files in the outputs directory.\"\"\"\n",
    "    output_dir = 'outputs'\n",
    "    if os.path.exists(output_dir):\n",
    "        pattern = os.path.join(output_dir, \"*.csv\")\n",
    "        csv_files = sorted(glob.glob(pattern))\n",
    "        print(f\"Found {len(csv_files)} CSV files in {output_dir}\")\n",
    "        return csv_files\n",
    "    else:\n",
    "        print(f\"Output directory {output_dir} does not exist\")\n",
    "        return []\n",
    "\n",
    "# Find all CSV files\n",
    "csv_files = find_csv_files()\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found. Please ensure your inference has been run and files are saved.\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(csv_files)} CSV files:\")\n",
    "    for file in csv_files:\n",
    "        print(f\"  - {file}\")\n",
    "\n",
    "    # Read and combine all CSV files\n",
    "    all_data = []\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            # Add source file information\n",
    "            df['source_file'] = os.path.basename(csv_file)\n",
    "            all_data.append(df)\n",
    "            print(f\"Loaded {len(df)} rows from {os.path.basename(csv_file)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "\n",
    "    if all_data:\n",
    "        # Combine all data\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\nCombined dataset has {len(combined_df)} rows\")\n",
    "        \n",
    "        # Display column names to verify structure\n",
    "        print(f\"Columns: {list(combined_df.columns)}\")\n",
    "        \n",
    "        # Check if in_range column exists\n",
    "        if 'in_range' not in combined_df.columns:\n",
    "            print(\"Warning: 'in_range' column not found. Available columns:\", list(combined_df.columns))\n",
    "        else:\n",
    "            # Calculate accuracy using in_range values\n",
    "            evaluation_results = {}\n",
    "            \n",
    "            for _, row in combined_df.iterrows():\n",
    "                file_name = row['source_file']\n",
    "                model = row['model'] if 'model' in row else 'unknown'\n",
    "                prompt_type = row['prompt_type'] if 'prompt_type' in row else 'unknown'\n",
    "                category = row['category'] if 'category' in row else 'unknown'\n",
    "                \n",
    "                # Special handling for weeks/days category due to special datatype handling\n",
    "                if category == 'date' and \")\" in str(row.get('parsed_answer', '')):\n",
    "                    ground_truth = parse_answer(row.get('ground_truth', ''), category)\n",
    "                    accuracy = 1 if str(ground_truth).strip() == str(row['parsed_answer']).strip() else 0\n",
    "\n",
    "                else:\n",
    "                    accuracy = 1 if row['in_range'] else 0\n",
    "                \n",
    "                # Create unique key combining model and prompt_type\n",
    "                key = f\"{model}_{prompt_type}\"\n",
    "                \n",
    "                if key not in evaluation_results:\n",
    "                    evaluation_results[key] = {}\n",
    "                \n",
    "                if category not in evaluation_results[key]:\n",
    "                    evaluation_results[key][category] = {'accuracy': []}\n",
    "                \n",
    "                evaluation_results[key][category]['accuracy'].append(accuracy)\n",
    "            \n",
    "            print(\"Calculated accuracy scores using in_range values.\")\n",
    "            \n",
    "            # Aggregate results\n",
    "            aggregated_results = []\n",
    "            \n",
    "            for key, categories in evaluation_results.items():\n",
    "                for category, scores in categories.items():\n",
    "                    # Calculate average accuracy\n",
    "                    avg_accuracy = sum(scores['accuracy']) / len(scores['accuracy']) if scores['accuracy'] else 0\n",
    "                    \n",
    "                    aggregated_results.append({\n",
    "                        'Model_Prompt': key,\n",
    "                        'Category': category,\n",
    "                        'Average Accuracy': avg_accuracy,\n",
    "                        'Sample Count': len(scores['accuracy'])\n",
    "                    })\n",
    "            \n",
    "            results_df = pd.DataFrame(aggregated_results)\n",
    "            \n",
    "            # Calculate overall averages for each model-prompt combination\n",
    "            overall_results = results_df.groupby('Model_Prompt')[['Average Accuracy']].mean().reset_index()\n",
    "            overall_results['Category'] = 'Overall'\n",
    "            overall_results['Sample Count'] = results_df.groupby('Model_Prompt')['Sample Count'].sum().values\n",
    "            \n",
    "            # Append overall results to the aggregated results\n",
    "            aggregated_results_with_overall = pd.concat([results_df, overall_results], ignore_index=True)\n",
    "            \n",
    "            # Pivot the table to have categories as columns, showing average accuracy\n",
    "            pivot_results_df = aggregated_results_with_overall.pivot_table(\n",
    "                index='Model_Prompt',\n",
    "                columns='Category',\n",
    "                values='Average Accuracy',\n",
    "                fill_value=0\n",
    "            ).reset_index()\n",
    "            \n",
    "            print(\"\\n=== ACCURACY RESULTS (Using in_range values) ===\")\n",
    "            display(pivot_results_df)\n",
    "            \n",
    "            # Also show sample counts\n",
    "            sample_counts_df = aggregated_results_with_overall.pivot_table(\n",
    "                index='Model_Prompt',\n",
    "                columns='Category',\n",
    "                values='Sample Count',\n",
    "                fill_value=0\n",
    "            ).reset_index()\n",
    "            \n",
    "            print(\"\\n=== SAMPLE COUNTS ===\")\n",
    "            display(sample_counts_df)\n",
    "            \n",
    "            # Summary statistics\n",
    "            print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "            summary_stats = []\n",
    "            for key in evaluation_results.keys():\n",
    "                overall_acc = []\n",
    "                for category in evaluation_results[key].values():\n",
    "                    overall_acc.extend(category['accuracy'])\n",
    "                \n",
    "                summary_stats.append({\n",
    "                    'Model_Prompt': key,\n",
    "                    'Overall Accuracy': sum(overall_acc) / len(overall_acc) if overall_acc else 0,\n",
    "                    'Total Samples': len(overall_acc)\n",
    "                })\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_stats)\n",
    "            display(summary_df)\n",
    "    else:\n",
    "        print(\"No data could be loaded from CSV files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Visualization and Analysis\n",
    "\n",
    "**Visualization Strategy:**\n",
    "The following section generates comprehensive visualizations of model performance across different prompting methods and model sizes.\n",
    "\n",
    "#### Visualization Types:\n",
    "- **Barplot**: Overall accuracy for each Model × Prompt combination\n",
    "- **Heatmap**: Per-category accuracy for each Model × Prompt combination  \n",
    "- **Radar Charts**: Category-wise performance comparison split by model size\n",
    "\n",
    "#### Analysis Insights:\n",
    "- **Model size impact**: Compare 0.6B vs 1.7B performance across methods\n",
    "- **Prompting effectiveness**: Identify which methods work best for different categories\n",
    "- **Category-specific patterns**: Understand model strengths/weaknesses by clinical calculation type\n",
    "- **Method comparison**: Direct comparison of zero-shot, few-shot, CoT, RAG, and fine-tuning\n",
    "\n",
    "**Output Files:**\n",
    "All plots are saved as PNG files in the `images/` directory for easy inclusion in reports and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Accuracy by Model/Prompt and Category, Save as PNGs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Ensure output directory for images exists\n",
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "# --- Barplot: Overall Accuracy by Model/Prompt ---\n",
    "if 'overall_results' in globals():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(\n",
    "        data=overall_results,\n",
    "        x='Model_Prompt',\n",
    "        y='Average Accuracy',\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.title('Overall Accuracy by Model and Prompt')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.xlabel('Model + Prompt')\n",
    "    plt.ylim(0, 0.6)  # <-- Changed from 1 to 0.6\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/overall_accuracy.png', dpi=200)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('overall_results not found. Please run the evaluation aggregation cell first.')\n",
    "\n",
    "# --- Heatmap: Per-Category Accuracy by Model/Prompt ---\n",
    "if 'pivot_results_df' in globals():\n",
    "    # Remove 'Overall' column if present for per-category heatmap\n",
    "    heatmap_data = pivot_results_df.set_index('Model_Prompt').drop(columns=['Overall'], errors='ignore')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='YlGnBu', vmin=0, vmax=0.6)  # <-- Changed vmax to 0.6\n",
    "    plt.title('Accuracy by Model/Prompt and Category')\n",
    "    plt.ylabel('Model + Prompt')\n",
    "    plt.xlabel('Category')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/category_accuracy_heatmap.png', dpi=200)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('pivot_results_df not found. Please run the evaluation aggregation cell first.')\n",
    "\n",
    "\n",
    "# --- Radar Chart: Per-Category Accuracy by Model/Prompt, Split by Model ---\n",
    "\n",
    "if 'pivot_results_df' in globals():\n",
    "    # Remove 'Overall' column if present\n",
    "    radar_data = pivot_results_df.set_index('Model_Prompt').drop(columns=['Overall'], errors='ignore')\n",
    "    categories = radar_data.columns.tolist()\n",
    "    N = len(categories)\n",
    "    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # complete the loop\n",
    "\n",
    "    # 1.7B models\n",
    "    model_17_keys = [k for k in radar_data.index if k.startswith('1.7B')]\n",
    "    if model_17_keys:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for label in model_17_keys:\n",
    "            values = radar_data.loc[label].tolist()\n",
    "            values += values[:1]\n",
    "            plt.polar(angles, values, label=label, linewidth=2, marker='o', alpha=0.7)\n",
    "        plt.xticks(angles[:-1], categories, fontsize=12)\n",
    "        plt.yticks(np.linspace(0, 0.6, 7), [f\"{x:.1f}\" for x in np.linspace(0, 0.6, 7)], color=\"grey\", size=10)\n",
    "        plt.ylim(0, 0.6)\n",
    "        plt.title('Per-Category Accuracy (Radar Chart) - Qwen3-1.7B', size=16, y=1.08)\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('images/category_accuracy_radar_1.7B.png', dpi=200, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    # 0.6B models\n",
    "    model_06_keys = [k for k in radar_data.index if k.startswith('0.6B')]\n",
    "    if model_06_keys:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for label in model_06_keys:\n",
    "            values = radar_data.loc[label].tolist()\n",
    "            values += values[:1]\n",
    "            plt.polar(angles, values, label=label, linewidth=2, marker='o', alpha=0.7)\n",
    "        plt.xticks(angles[:-1], categories, fontsize=12)\n",
    "        plt.yticks(np.linspace(0, 0.6, 7), [f\"{x:.1f}\" for x in np.linspace(0, 0.6, 7)], color=\"grey\", size=10)\n",
    "        plt.ylim(0, 0.6)\n",
    "        plt.title('Per-Category Accuracy (Radar Chart) - Qwen3-0.6B', size=16, y=1.08)\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('images/category_accuracy_radar_0.6B.png', dpi=200, bbox_inches='tight')\n",
    "        plt.show()\n",
    "else:\n",
    "    print('pivot_results_df not found. Please run the evaluation aggregation cell first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Export and Documentation\n",
    "\n",
    "**Output Files:**\n",
    "This section exports all evaluation results to CSV files for downstream analysis and reporting:\n",
    "\n",
    "#### Core Results Files:\n",
    "- **`aggregated_results.csv`**: Detailed per-run category accuracy scores\n",
    "- **`overall_results.csv`**: Overall accuracy by Model × Method combination\n",
    "- **`pivot_results.csv`**: Wide-format table for easy comparison and visualization\n",
    "- **`sample_counts.csv`**: Sample counts by category for statistical significance\n",
    "- **`summary_statistics.csv`**: Global performance rollups and key metrics\n",
    "\n",
    "#### File Usage:\n",
    "- **Research analysis**: Import into analysis tools for further statistical analysis\n",
    "- **Report generation**: Direct inclusion in papers, presentations, and documentation\n",
    "- **Reproducibility**: Complete results export for verification and comparison\n",
    "- **README integration**: Key metrics for project documentation and GitHub display\n",
    "\n",
    "**Data Structure:**\n",
    "Each CSV maintains consistent column naming and data types to ensure compatibility across different analysis workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Findings\n",
    "\n",
    "### Methodology Overview\n",
    "This notebook implements a comprehensive evaluation of clinical reasoning models using multiple approaches:\n",
    "\n",
    "1. **Prompt Engineering**: Zero-shot, few-shot, chain-of-thought, and RAG methods\n",
    "2. **Parameter-Efficient Fine-Tuning**: LoRA/QLoRA with 4-bit quantization\n",
    "3. **Model Comparison**: Qwen3-0.6B vs Qwen3-1.7B across all methods\n",
    "4. **Category-Wise Analysis**: Performance breakdown by clinical calculation type\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "#### Prompt Design Rationale\n",
    "- **Zero-shot**: Direct instruction-following with category-specific formatting\n",
    "- **Few-shot**: K=3 examples for context/VRAM balance\n",
    "- **CoT**: Structured reasoning with `<thinking>` tags for transparency\n",
    "- **RAG**: Dynamic example retrieval using semantic similarity\n",
    "\n",
    "#### LoRA/QLoRA Configuration\n",
    "- **Rank 16**: Sufficient capacity without overfitting\n",
    "- **4-bit NF4**: Memory efficiency while preserving quality\n",
    "- **Target modules**: Attention and MLP layers for comprehensive adaptation\n",
    "- **Training**: Conservative learning rate (2e-5) for stable fine-tuning\n",
    "\n",
    "### Expected Results Patterns\n",
    "- **Model size**: 1.7B generally outperforms 0.6B across methods\n",
    "- **Method ranking**: RAG outperformed all other approaches, CoT, few-shot and fine-tuning acheived similar results, zero-shot overall performed the worst\n",
    "- **Category differences**: Numeric calculations typically easier than date/gestational age\n",
    "- **Efficiency trade-offs**: Larger models and complex methods require more resources, these lightweight models sacrifced performance for minimal resources\n",
    "\n",
    "### Reproducibility Features\n",
    "- **Complete parameter documentation**: All choices explained with rationale\n",
    "- **Modular design**: Each section can be run independently\n",
    "- **Output standardization**: Consistent CSV format for all results\n",
    "- **Visualization**: Comprehensive plots for result interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated results to CSV\n",
    "results_df.to_csv(\"aggregated_results.csv\", index=False)\n",
    "overall_results.to_csv(\"overall_results.csv\", index=False)\n",
    "pivot_results_df.to_csv(\"pivot_results.csv\", index=False)\n",
    "sample_counts_df.to_csv(\"sample_counts.csv\", index=False)\n",
    "summary_df.to_csv(\"summary_statistics.csv\", index=False)\n",
    "\n",
    "print(\"All evaluation results saved as CSV files:\")\n",
    "print(\" - aggregated_results.csv\")\n",
    "print(\" - overall_results.csv\")\n",
    "print(\" - pivot_results.csv\")\n",
    "print(\" - sample_counts.csv\")\n",
    "print(\" - summary_statistics.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2193aa8aa26f4f6098638667ff4c23e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23524eeb480b48dca2d7be051d60ef5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28b118d7850d41d7a1ad8d473adbb3be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bbc6e995c1f4f63be50f5e9dad6e376": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2def2041cb114c1db533ead31a234748": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4daff3b9a2af4cdcb26300d230ecfc94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd985c75884a4ba5992726c376a0f0b8",
      "placeholder": "​",
      "style": "IPY_MODEL_23524eeb480b48dca2d7be051d60ef5a",
      "value": " 10053/10053 [00:33&lt;00:00, 758.77 examples/s]"
     }
    },
    "5a186ceda6324233860986498222e2ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28b118d7850d41d7a1ad8d473adbb3be",
      "max": 1047,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0f347bff23b40ae900077846b5441e7",
      "value": 0
     }
    },
    "6ef7346f78a444a282edf65a4c2479e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae1bc63b4e8c42b896f6c9999bcde18b",
      "max": 10053,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2def2041cb114c1db533ead31a234748",
      "value": 10053
     }
    },
    "7217115a6f304797919d81a23968b486": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "801b36af9e7045e596f8b719cef3e3f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9186461e305f4e80ba48cbd1897d7688": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d20d912668245d483fb35e33a2d9555": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccac27e3c2bf45e7af1e81fe577e32ed",
      "placeholder": "​",
      "style": "IPY_MODEL_7217115a6f304797919d81a23968b486",
      "value": "Map: 100%"
     }
    },
    "acae790e32c74a82bd4c8b05f463e1f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bebd4dc4632d48b784a1f2910a533114",
      "placeholder": "​",
      "style": "IPY_MODEL_9186461e305f4e80ba48cbd1897d7688",
      "value": " 0/1047 [00:01&lt;?, ?it/s]"
     }
    },
    "ae1bc63b4e8c42b896f6c9999bcde18b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0f347bff23b40ae900077846b5441e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bebd4dc4632d48b784a1f2910a533114": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccac27e3c2bf45e7af1e81fe577e32ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd985c75884a4ba5992726c376a0f0b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd08890a2bcc4bea9e1d8f295ad48c9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2193aa8aa26f4f6098638667ff4c23e2",
      "placeholder": "​",
      "style": "IPY_MODEL_fce5a57cb99049fa9fe98c4c9cf94c15",
      "value": "0.6B-few_shot:   0%"
     }
    },
    "e4862d4872a1492088c2a3a0d943ae74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d20d912668245d483fb35e33a2d9555",
       "IPY_MODEL_6ef7346f78a444a282edf65a4c2479e6",
       "IPY_MODEL_4daff3b9a2af4cdcb26300d230ecfc94"
      ],
      "layout": "IPY_MODEL_801b36af9e7045e596f8b719cef3e3f0"
     }
    },
    "e6e7ca43d838432bb6d10b13d5a947fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd08890a2bcc4bea9e1d8f295ad48c9d",
       "IPY_MODEL_5a186ceda6324233860986498222e2ae",
       "IPY_MODEL_acae790e32c74a82bd4c8b05f463e1f9"
      ],
      "layout": "IPY_MODEL_2bbc6e995c1f4f63be50f5e9dad6e376"
     }
    },
    "fce5a57cb99049fa9fe98c4c9cf94c15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
